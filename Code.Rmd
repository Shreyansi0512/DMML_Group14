---
title: "Predicting Effectiveness of Bank Marketing"
author: "Group 14: Mahima Mago, Ninad Khare, Shreyansi Jain, Subasish Behera"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
    df_print: paged
fig_caption: yes
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(gmodels) # Cross Tables [CrossTable()]
library(ggmosaic) # Mosaic plot with ggplot [geom_mosaic()]
library(corrplot) # Correlation plot [corrplot()]
library(ggpubr) # Arranging ggplots together [ggarrange()]
library(cowplot) # Arranging ggplots together [plot_grid()]
library(caret) # ML [train(), confusionMatrix(), createDataPartition(), varImp(), trainControl()]
library(ROCR) # Model performance [performance(), prediction()]
library(plotROC) # ROC Curve with ggplot [geom_roc()]
library(pROC) # AUC computation [auc()]
library(PRROC) # AUPR computation [pr.curve()]
library(rpart) # Decision trees [rpart(), plotcp(), prune()]
library(rpart.plot) # Decision trees plotting [rpart.plot()]
library(ranger) # Optimized Random Forest [ranger()]
library(lightgbm) # Light GBM [lgb.train()]
library(xgboost) # XGBoost [xgb.DMatrix(), xgb.train()]
library(MLmetrics) # Custom metrics (F1 score for example)
library(tidyverse) # Data manipulation
#library(doMC) # Parallel processing
library(janitor)
library(gbm) #For gradient boosting
library(caret)
```


```{r}
data = read.csv("group_14.csv")
data=replace(data,data=='',"missing")
head(data)
data %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "variable", value = "no_missing") %>% 
  arrange(-no_missing)

#Defining crosstable to be used in our analysis
Crosstab = function(df, var1, var2){
  # df: dataframe containing both columns to cross
  # var1, var2: columns to cross together.
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

```

```{r}
#Treating missing values

#Default variable
table(data$default)

#As we can see from the table, only 1 individual replied with "yes", that they have credit in default. 
#78.5% individuals answered "no" and 21.5% did not reply at all. Hence, this variable is of not much significance as we get no information from it. Thus, we would remove this from our dataset.

data2=data %>%
  select(-default)

#Education 

Crosstab(data2,"education","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data2%>%
  filter(education!="missing")

#Housing

Crosstab(data2,"housing","y")

chisq.test(data2$housing,data2$y)

#Since, the chi-square test for this variable has p-value less than 0.05 there is a significant association with the response variable. Hence, we would keep this in our dataset.

#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data2%>%
  filter(housing!="missing")

#Loan

Crosstab(data2,"loan","y")
chisq.test(data2$loan,data2$y)

#Since, the chi-square test for this variable has p-value more than 0.05 there is no significant association with the response variable. Hence, we would exclude this from our dataset.

data2= data2 %>%
  select(-loan)

#Job
table(data2$job)

Crosstab(data2,"job","y")

#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data2%>%
  filter(job!="missing")

#Marital status

Crosstab(data2,"marital","y")

#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data2%>%
  filter(marital!="missing")
dim(data2)
dim(data)

data2 %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "variable", value = "no_missing") %>% 
  arrange(-no_missing)

#No missing values left in our final dataset.
```

```{r}
#Train-test split.Using 80%/20% split.
set.seed(123)

n=nrow(data2)
ind=sample(c(1:n),0.8*n)
data.train=data2[ind,]
data.test=data2[-ind,]

dim(data.train)
dim(data.test)
```

```{r}
#EXPLORATORY DATA ANALYSIS
#age
data.train %>% ggplot(aes(age)) +
  geom_histogram(color = 'white')
data.train %>% ggplot(aes(age, color = factor(y))) + 
  geom_density() +
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  guides(color = guide_legend(title = 'Subscribed'))
#age group 30-58 more likely to say no. and more importantly
#product was subscribed with high likelihood after the age of ~60


#job
data.train %>% count(job) %>%
  ggplot(aes(y = fct_reorder(job, n), x = n))+
  geom_col() +
  labs(x = 'Count', y = 'Job') + 
  scale_x_continuous(breaks = seq(0, 2000, 200))

##comparison of proportion of 'yes' across job types
data.train %>% ggplot(aes(y = fct_rev(fct_infreq(job)), fill = y))+
     geom_bar(position = 'fill', color = 'black') +
     scale_fill_brewer(palette = 'Pastel1') +
     labs(x = 'proportion', y = 'Job')

#the high proportion of 'retired' job category resulting in 'yes'
#supports our previous observation related to age variable.


#marital
data.train %>% count(marital, y) %>% ungroup() %>%
  group_by(marital) %>% mutate(prop = n / sum(n)) %>%
  pivot_wider(names_from = y, values_from = c(n, prop))

data.train %>% ggplot(aes(x = marital, fill = y)) +
  geom_bar(position = 'fill', color = 'black') +
  scale_fill_brewer(palette = 'Pastel1')
#the proportion that resulted in 'yes' is not markedly different across marital status.


#education
data.train %>% ggplot(aes(y = education, y)) +
  geom_count() +
  labs(y = 'Education', x = 'count')
#Count of 'yes' for people with 'University degree' seems to be high

data.train %>% ggplot(aes(y = education, fill = y)) +
  geom_bar(position = 'fill', color = 'black') +
  scale_fill_brewer(palette = 'Pastel1')
#the proportion related to illiterate category is misleading because
#the overall sample size is very small
#other than that, 'university degree' and 'professional course' have a
#higher proportion


#housing
data.train %>% count(housing, y)

data.train %>% ggplot(aes(housing, fill = y)) +
  geom_bar(position = 'fill', color = 'black')+
  scale_fill_brewer(palette = 'Pastel1')

```



```{r}
#Contact
data.train %>% 
  ggplot() +
  aes(x = contact, y = after_stat(count)/nrow(data.train), fill = y) +
  geom_bar()

#Here, we can see that there have been more term deposits from cellular responders, 14.5% as compared to telephone responders which is just 5.6%.

#Month

Crosstab(data.train,"month","y")

data.train=data.train %>%
  mutate(month=factor(data.train$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))

data.train %>% 
  ggplot() +
  aes(x = month, y = after_stat(count)/nrow(data.train), fill = y) +
  geom_bar()

#From the barplot, we can see that there has been no contact in the months of January and February. Most contact has been made in May almost 33.7%, but most of the people declined to deposit here as well. Least contact has been made in December.

#Day of the week

Crosstab(data.train,"day_of_week","y")

data.train=data.train %>%
  mutate(day_of_week=factor(data.train$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

data.train %>% 
  ggplot() +
  aes(x = day_of_week, y = after_stat(count)/nrow(data.train), fill = y) +
  geom_bar()

#Duration

data.train=data.train %>%
  select(-duration)

#There doesn't seem to be a significant relationship between our response variable and duration. As our goal is to know how many people will subscribe to the term deposit, it is not really possible to know the duration of the call beforehand.


#Campaign

data.train %>%
  ggplot()+
  aes(x=campaign)+
  geom_bar()

#We can see that for more than 15 calls, every responder has declined for the term deposit. If we look at the data only for a total a 15 calls-

data.train %>%
  filter(campaign<=15) %>%
  ggplot()+
  aes(x=campaign)+
  geom_bar()

data.train = data.train %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

```

```{r}

# Pdays : Number of days passed after client was last contacted by a previous campaign

# Distribution of data
table(data.train$pdays)

# The number 999 means that the client has not been contacted before, this constitutes as the majority of the clients. 
# This variable can be used to check if previous contact with a client is beneficial, but we need to convert this variable

data.train = data.train %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              select(-pdays)

Crosstab(data.train,'n_contact','y')

chisq.test(data.train$n_contact,data.train$y)
# It seems that when a client is contacted before,they are more receptive towards saying Yes
# This is verified with the chisq test as well.

# previous : Number of times a client has been contacted before this campaign
table(data.train$previous)

# This variable will provide the same information after encoding it to whether or not a previous contact has been made with the client or not.

# Poutcome : Outcome of the previous campaign on the respective client.


table(data.train$poutcome)

Crosstab(data.train,'poutcome','y')

```

```{r}
# Exploratory Data Analysis

# All Continuous variables
#Histogram and density plot of the numerical variables

summary(data.train)

num_variables = c('age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')
library(GGally)

ggpairs(data.train,columns = num_variables)

corrplot(cor(data.train[num_variables]))

for (col in num_variables){
  
  plot = ggplot(data = data.train,aes(x = data.train[,col])) +
    geom_histogram() + labs(x = col)
  print(plot)
}


for(col in num_variables){
  plot <- ggplot(aes(y = data.train[, col]), data = data.train)+
    geom_boxplot(aes(color = y)) + labs(y = col, x = 'y')
  print(plot)
}
#boxplot for euribor3m, and emp.var.rate display some
#pattern

#the density plot for the two variables
data.train %>% ggplot(aes(x = euribor3m)) +
  geom_density(aes(color = y))

data.train %>% ggplot(aes(x = emp.var.rate)) +
  geom_density(aes(color = y))

#density plot for euribor3m shows some inherent difference
#between categories

#boxplot for emp.var.rate is misleading


Crosstab(data.train,"contact","y")
```

```{r}
data.train.num <- data.train %>%
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.train.char <- data.train %>%
  select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

min = sapply(data.train.num,min)
range = sapply(data.train.num, range)

data.train.num = (data.train.num-min)/ range


data.train.char <- data.train.char %>%
  map_dfc(as.factor)

data.train.scaled <- cbind(data.train %>% select(age, month, day_of_week, previous), data.train.char, data.train.num)

data.train.scaled <- data.train.scaled %>% 
  select(order(colnames(data.train.scaled)))

data.train.scaled %>% head()
```


```{r Test data}
data.test = data.test %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              select(-pdays)

data.test=data.test %>%
  mutate(month=factor(data.test$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))
data.test=data.test %>%
  mutate(day_of_week=factor(data.test$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

data.test = data.test %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

data.test.num <- data.test %>%
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.test.char <- data.test %>%
  select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

data.test.num = (data.test.num-min)/ range

data.test.char <- data.test.char %>%
  map_dfc(as.factor)

data.test.scaled <- cbind(data.test %>% select(age, month, day_of_week,previous), data.test.char, data.test.num)

data.test.scaled <- data.test.scaled %>% 
  select(order(colnames(data.test.scaled)))

data.test.scaled %>% head()
names(data.test.scaled)
names(data.train.scaled)

```



```{r}
# Ninad Model Fitting : SVM

library(e1071)


# Values for hyperparameters
set.seed(123)
cost_range <- c(0.01,0.1,1,2,5,10)
degree_range <- 2:4
gamma_range <- c(0.001,0.01,0.1,1,10)

# Since the tune.svm() function is too computationally expensive, we will split our training dataset again into training and validation set to tune hyperparameters for different kernels

# Index for splitting data
n = nrow(data.train.scaled)
sub_idx = sample(c(1:n),n*0.15)

# Function for calculating prediction error.

pred.error<-function(pred,truth){
mean(pred!=truth)
}

# SVM with linear kernel:

C.error <- numeric(length(cost_range))

for(i in 1:length(cost_range)){
  model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="linear",cost =cost_range[i])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i] <- pred.error(pred.model,data.train.scaled$y)
}

cost_selected = cost_range[which.min(C.error)]

optim.svm.linear = svm(y~.,data = data.train.scaled[-sub_idx], type="C-classification", kernel="linear",cost = cost_selected)


data.train.pred = predict(optim.svm.linear,data.train.scaled[-sub_idx,])
print("Table for linear SVM training data:")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.linear,data.train.scaled[sub_idx,])
print("Table for linear SVM validation data:")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# SVM with polynomial kernel

C.error = matrix(nrow = length(degree_range),ncol = length(cost_range))

for(i in 1:length(degree_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

degree_selected = degree_range[which(C.error == min(C.error), arr.ind=TRUE)[,'row']]

cost_selected = cost_range[which(C.error == min(C.error), arr.ind=TRUE)[,'col']]


optim.svm.poly = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.poly,data.train.scaled[-sub_idx,])
print("Table for poly SVM training data")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.poly,data.train.scaled[sub_idx,])
print("Table for poly SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# SVM with Radial kernel

C.error = matrix(nrow = length(gamma_range),ncol = length(cost_range))


for(i in 1:length(gamma_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

gamma_selected = gamma_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'row'])]

cost_selected = cost_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'col'])]

optim.svm.radial = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.radial,data.train.scaled[-sub_idx,])
print("Table for radial SVM training data")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.radial,data.train.scaled[sub_idx,])
print("Table for radial SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# The linear and polynomial SVM perform much better than the radial kernel, the radial kernel either overfits the data or classifies all values as 'No' so even with good accuracy , it is not at all a good fit for the data.
```

```{r kNN Technique}

#Preparing the data for kNN

data.kNN.char = data.train.scaled %>%
  select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char = data.kNN.char %>%
  map_dfc(as.numeric)

data.kNN <- cbind(data.train.scaled %>% select(age,previous), data.kNN.char, data.train.num)

data.kNN = data.kNN %>%
  select(order(colnames(data.kNN)))

glimpse(data.kNN)

N=nrow(data.kNN)
div=sample(c(1:N),0.8*N)
data.train.kNN=data.kNN[div,]
data.val.kNN=data.kNN[-div,]

glimpse(data.train.kNN)

classes = data.train.kNN[,18]
classes
classes.val=data.val.kNN[,18]
classes.val

glimpse(data.train.kNN)

#For test data

data.kNN.char.test = data.test.scaled %>%
  select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char.test
data.kNN.char.test = data.kNN.char.test %>%
  map_dfc(as.numeric)

data.test.kNN <- cbind(data.test.scaled %>% select(age,previous), data.kNN.char.test, data.test.num)

glimpse(data.test.kNN)

data.test.kNN = data.test.kNN %>%
  select(order(colnames(data.test.kNN)))

#K-fold cross validation, to find the right value for k

K=c(1:15)
val.error=c()
for (k in K) {
  val.pred=knn(data.train.kNN, data.val.kNN, classes, k=k)
  val.error[k]=mean(classes.val != val.pred)
}

plot(K, val.error, type="b", ylab="validation error rate")

k.opt <- which.min(val.error)
k.opt

#Predicting the values using train data

train.pred=knn(data.train.kNN, data.val.kNN, classes, k=k.opt)
train.tab=table(data.val.kNN[,18],train.pred)
train.rate=prop.table(train.tab,margin=1)
train.rate
train.tab

Sensitivity=train.tab[2,2]/(train.tab[2,2]+train.tab[2,1])
Sensitivity
Accuracy= (train.tab[1,1]+train.tab[2,2])/N
Accuracy

#Leave one-out-cross validation

K <- c(1:15)
cv.error <- c()
for (k1 in K){
  train.pred <- knn.cv(data.kNN, data.kNN[,18], k=k1)
  cv.error[k1] <- mean(data.kNN[,18] != train.pred)
}

plot(K, cv.error, type="b", ylab="validation error rate")

k.opt2=which.min(cv.error)
k.opt2

train.pred2 <- knn(data.kNN, data.train.kNN, data.kNN[,18], k=k.opt2)
train.tab2=table(data.train.kNN[,18],train.pred2)
train.rate2=prop.table(tab.pred2,margin=1)
train.rate2

Sensitivity2=train.tab2[2,2]/(train.tab2[2,2]+train.tab2[2,1])
Sensitivity2
Accuracy2= (train.tab2[1,1]+train.tab2[2,2])/N
Accuracy2

#Comparing the two cross-validations
c(Sensitivity,Accuracy)
c(Sensitivity2,Accuracy2)

#Out of the two cross-validations, both the sensitivity and accuracy are better in leave one out cross validation. 

```

```{r Gradient bossting technique}

#Fitting the model on the training set

model.gbm=train(y~.,data=data.train.scaled,method="xgbTree", trControl=trainControl("cv",number=10))
model.gbm$bestTune

pred.class=model.gbm %>%
  predict(data.train.scaled)
pred.class

#Variable Importance
varImp(model.gbm)

#Compute model prediction accuracy rate
mean(pred.class==data.train.scaled[,18])
tab.gbm=table(pred.class,data.train.scaled[,18])
rate.gbm=prop.table(tab.gbm,margin = 1)
rate.gbm

sens=tab.gbm[2,2]/(tab.gbm[2,1]+tab.gbm[2,2])
sens

#Predicting on test data
pred.class.test=model.gbm %>%
  predict(data.test.scaled)
pred.class.test

mean(pred.class.test==data.test.scaled[,18])
tab.gbm.test=table(pred.class.test,data.test.scaled[,18])
rate.gbm=prop.table(tab.gbm.test,margin = 1)
rate.gbm

sens2=tab.gbm.test[2,2]/(tab.gbm.test[2,1]+tab.gbm.test[2,2])
sens2
```

```{r cleaning environment}
rm(data.test.char)
rm(data.test.num)
rm(data.train.char)
rm(data.train.num)
rm(data.test)
rm(data.train)
rm(plot)
rm(col)
rm(range)
rm(num_variables)
rm(ind)
rm(min)
rm(n)
```

```{r}
#Tree Based Methods- own way

tree <- rpart(y~ ., data = data.train.scaled, method = "class", minsplit = 4, minbucket = 2, cp = 0.04)
rpart.plot(tree, type = 2, extra = 106)

tree$variable.importance
n= ncol(data.train.scaled)

train.pred <- predict(tree, newdata= data.train.scaled,type="class")
table(data.train.scaled[,n], train.pred)

176/(176+661)
(6513+176)/(6513+92+661+176)

data.train.rt.pruned <- prune(tree, cp=0.04)
rpart.plot(data.train.rt.pruned)


```

```{r}
#Tree Based Methods

tune_grid = expand.grid(
  cp = seq(from = 0, to = 0.01, by = 0.001)
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = FALSE, # no training log
  allowParallel = FALSE, # FALSE for reproducible results 
  classProbs = TRUE
)

rpart1_tune = train(
  y ~ .,
  data = data.train.scaled,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "rpart")

ggplot(rpart1_tune) +
  theme(legend.position = "bottom")

tree <- rpart(y ~ ., data = data.train.scaled, cp = rpart1_tune$bestTune)

rpart.plot(tree)
```


```{r}
# plotting importance from predictive models into two panels
fun_imp_ggplot_split = function(model){
  # model: model used to plot variable importances
  
  if (class(model)[1] == "ranger"){
    imp_df = model$variable.importance %>% 
      data.frame("Overall" = .) %>% 
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  } else {
    imp_df = varImp(model) %>%
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  }
  
  # first panel (half most important variables)
  gg1 = imp_df %>% 
    slice(1:floor(nrow(.)/2)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("Variables") +
    ylab("Importance") +
    theme(legend.position = "none")
    
  imp_range = ggplot_build(gg1)[["layout"]][["panel_params"]][[1]][["x.range"]]
  imp_gradient = scale_fill_gradient(limits = c(-imp_range[2], -imp_range[1]),
                                     low = "#132B43", 
                                     high = "#56B1F7")
  
  # second panel (less important variables)
  gg2 = imp_df %>% 
    slice(floor(nrow(.)/2)+1:nrow(.)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("") +
    ylab("Importance") +
    theme(legend.position = "none") +
    ylim(imp_range) +
    imp_gradient
  
  # arranging together
  gg_both = plot_grid(gg1 + imp_gradient,
                      gg2)
  
  return(gg_both)
}


# plotting two performance measures
fun_gg_cutoff = function(score, obs, measure1, measure2) {
  # score: predicted scores
  # obs: real classes
  # measure1, measure2: which performance metrics to plot
  
  predictions = prediction(score, obs)
  performance1 = performance(predictions, measure1)
  performance2 = performance(predictions, measure2)
  
  df1 = data.frame(x = performance1@x.values[[1]],
                   y = performance1@y.values[[1]],
                   measure = measure1,
                   stringsAsFactors = F) %>% 
    drop_na()
  df2 = data.frame(x = performance2@x.values[[1]],
                   y = performance2@y.values[[1]],
                   measure = measure2,
                   stringsAsFactors = F) %>% 
    drop_na()
  
  # df contains all the data needed to plot both curves
  df = df1 %>% 
    bind_rows(df2)
    
  # extracting best cut for each measure
  y_max_measure1 = max(df1$y, na.rm = T)
  x_max_measure1 = df1[df1$y == y_max_measure1, "x"][1]
  
  y_max_measure2 = max(df2$y, na.rm = T)
  x_max_measure2 = df2[df2$y == y_max_measure2, "x"][1]
  
  txt_measure1 = paste("Best cut for", measure1, ": x =", round(x_max_measure1, 3))
  txt_measure2 = paste("Best cut for", measure2, ": x =", round(x_max_measure2, 3))
  txt_tot = paste(txt_measure1, "\n", txt_measure2, sep = "")
  
  # plotting both measures in the same plot, with some detail around.
  gg = df %>% 
    ggplot() +
    aes(x = x,
        y = y,
        colour = measure) +
    geom_line() +
    geom_vline(xintercept = c(x_max_measure1, x_max_measure2), linetype = "dashed", color = "gray") +
    geom_hline(yintercept = c(y_max_measure1, y_max_measure2), linetype = "dashed", color = "gray") +
    labs(caption = txt_tot) +
    theme(plot.caption = element_text(hjust = 0)) +
    xlim(c(0, 1)) +
    ylab("") +
    xlab("Threshold")
    
  return(gg)
}
  
# creating classes according to score and cut
fun_cut_predict = function(score, cut) {
  # score: predicted scores
  # cut: threshold for classification
  
  classes = score
  classes[classes > cut] = 1
  classes[classes <= cut] = 0
  classes = as.factor(classes)
  
  return(classes)  
}

fun_imp_ggplot_split(tree)
```


```{r}
tree_train_score = predict(tree,
                           newdata = data.train.scaled,
                           type = "prob")[, 2]

tree_test_score = predict(tree,
                          newdata = data.test.scaled,
                          type = "prob")[, 2]

measure_train = fun_gg_cutoff(tree_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.25, 0.5), 
             linetype = "dashed")
```
```{r}

data.train.scaled_1 = data.train.scaled %>% 
  mutate(y = factor(if_else(y == "yes", "1", "0"), 
                    levels = c("0", "1")))
tree_train_cut = 0.25
tree_train_class = fun_cut_predict(tree_train_score, tree_train_cut)
tree_train_confm = confusionMatrix(tree_train_class, data.train.scaled_1$y, 
                                   positive = "1",
                                   mode = "everything")
tree_train_confm

```


```{r}
#Random Forest

tune_grid = expand.grid(
  mtry = c(1:(floor(ncol(data.train.scaled) * 0.7))),
  splitrule = c("gini", "extratrees"),
  min.node.size = 1
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = FALSE, # no training log
  allowParallel = FALSE, # FALSE for reproducible results 
  classProbs = TRUE
)

ranger_tune = train(
  y ~ .,
  data = data.train.scaled,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "ranger"
)

str(data.train.scaled)

ggplot(ranger_tune) +
  theme(legend.position = "bottom")

rf = ranger(y ~ .,
            data = data.train.scaled,
            num.trees = 1000,
            importance = "impurity",
            splitrule = ranger_tune$bestTune$splitrule,
            mtry = ranger_tune$bestTune$mtry,
            min.node.size = ranger_tune$bestTune$min.node.size,
            write.forest = T,
            probability = T)

```





```{r}
print(rf)
```


```{r}
fun_imp_ggplot_split(rf)
```
```{r}
rf_train_score = predict(rf,
                         data = data.train.scaled)$predictions[, 2]

rf_test_score = predict(rf,
                        data = data.train.scaled)$predictions[, 2]

measure_train = fun_gg_cutoff(rf_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
measure_train = fun_gg_cutoff(rf_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
```
```{r}
rf_train_cut = 0.3
rf_train_class = fun_cut_predict(rf_train_score, rf_train_cut)
# matrix
rf_train_confm = confusionMatrix(rf_train_class, data.train.scaled_1$y, 
                                 positive = "1",
                                 mode = "everything")
rf_train_confm

```

```{r, eval=TRUE}
#LDAQDA

#data preparation to apply PCA to continuous correlated variables
data.train.num <- data.train.scaled %>% dplyr::select(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed)

data.train.num.pca <- princomp(data.train.num, cor = T)

print(summary(data.train.num.pca))
#the first component accounts for 99.8% variation

#atleast 80% variance to be retained.
#using cumulative proportion values to choose the components

data.train.scaledLda <- data.train.scaled %>% bind_cols(data.train.num.pca$scores[, 1:3]) %>% select(-c(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed))


```

```{r LDA, eval=TRUE}
library(MASS)

#start with a simpler model
#include the variables that deemed most imp. during exploratory analysis
lda.fit1 <- lda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)


```

```{r, eval=TRUE}

#dataframe containing actual, predicted values and decision scores
#all evaluated on training set.
#Cross validation to be done at a later stage
predict.data <- data.train.scaledLda %>% dplyr::select(y)
predict.data['y_lda1'] <- predict(lda.fit1)$class
predict.data['y_lda1_probs'] <- predict(lda.fit1)$posterior[, 'yes']

confusionMatrix(predict.data$y_lda1, predict.data$y, positive = 'yes', mode = 'everything')

```



```{r autoeval, eval=TRUE}
#auto_eval() will add variables to the model one by one and compare the model's
#sensitivity and precision. If any added variable gives improvement of over 1% point,
#it will display the formula for that model. It does not stop iteration if error
#occurs during model fitting

auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  lda1_sensi <- 0.21
  lda1_preci <- 0.656
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(lda.fit.trial <- lda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > lda1_sensi + 0.1 | preci > lda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```


After the first model, a trial and error approach was taken to select the variables to add to the model. None of the variables provided better prediction with respect to all the metrics. Hence the optimal Linear discriminant model was found to be lda.fit1

```{r QDA, eval=TRUE}
#the initial QDA model uses the same variables as LDA
qda.fit1 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)
```

```{r predictdataqda, eval=TRUE}

#data frame containing predicted, actual values and decision scores
#for qda
predict.data['y_qda1'] <- predict(qda.fit1)$class
predict.data['y_qda1_probs'] <- predict(qda.fit1)$posterior[, 'yes']

confusionMatrix(predict.data$y_qda1, predict.data$y, positive = 'yes', mode = 'everything')
```
Same trial and error process was adopted.

```{r, eval=TRUE}

auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  qda1_sensi <- 0.217
  qda1_preci <- 0.65
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(qda.fit.trial <- qda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > qda1_sensi + 0.1 | preci > qda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```

```{r qda.fit2, eval=TRUE}
#job variable detected by auto_eval
#job variable added
qda.fit2 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome + job, data = data.train.scaledLda)

```

```{r, eval=TRUE}
predict.data['y_qda2'] <- predict(qda.fit2)$class
predict.data['y_qda2_probs'] <- predict(qda.fit2)$posterior[, 'yes']

confusionMatrix(predict.data$y_qda2, predict.data$y, positive = 'yes', mode = 'everything')

#Sensitivity for qda.fit2 increased but precision declined; Precision recall trade-off
```

.

```{r CV , eval=TRUE}

#cross validation results for all three models i.e.
#lda.fit1, qda.fit1, qda.fit2

set.seed(169)
folds <- createFolds(data.train.scaledLda$y, k = 10)

lda1_cv_sensi <- vector(mode = 'numeric', length = 10)
lda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda1_cv_sensi <- vector(mode = 'numeric', 10)
qda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda2_cv_sensi <- vector('numeric', 10)
qda2_cv_preci <- vector('numeric', 10)

for(i in 1:length(folds)){
  train <- data.train.scaledLda[-folds[[i]], ]
  test <- data.train.scaledLda[folds[[i]], ]
  
  lda1 <- lda(formula(lda.fit1$call), data = train)
  qda1 <- qda(formula(qda.fit1$call), data = train)
  qda2 <- qda(formula(qda.fit2$call), data = train)
  
  lda1_cv_sensi[i] <- recall(predict(lda1, test)$class, test$y, relevant = 'yes')
  lda1_cv_preci[i] <- precision(predict(lda1, test)$class, test$y, relevant = 'yes')
  
  qda1_cv_sensi[i] <- recall(predict(qda1, test)$class, test$y, relevant = 'yes')
  qda1_cv_preci[i] <- precision(predict(qda1, test)$class, test$y, relevant = 'yes')
  
  qda2_cv_sensi[i] <- recall(predict(qda2, test)$class, test$y, relevant = 'yes')
  qda2_cv_preci[i] <- precision(predict(qda2, test)$class, test$y, relevant = 'yes')
              
}

metric.frame <- data.frame(avg_sensi = c(mean(lda1_cv_sensi), mean(qda1_cv_sensi), mean(qda2_cv_sensi)),
                           avg_preci = c(mean(lda1_cv_preci), mean(qda1_cv_preci), mean(qda2_cv_preci)),
                           row.names = c('lda1', 'qda1', 'qda2'))

metric.frame
```

