---
title: "Predicting Effectiveness of Bank Marketing"
author: "Group 14: Mahima Mago, Ninad Khare, Shreyansi Jain, Subasish Behera"
output:
  pdf_document:
          latex_engine: xelatex
          number_sections: yes
          toc: yes
bibliography: References.bib  
fig_caption: yes
---

```{r setup, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, eval = TRUE, include = TRUE, comment = NA, message = FALSE)
```

```{r libraries, include = FALSE}
#Loading the required packages

library(gmodels) # Cross Tables [CrossTable()]
library(caret) # ML [train(), confusionMatrix(), createDataPartition(), varImp(), trainControl()]
library(ROCR) # Model performance [performance(), prediction()]
library(rpart) # Decision trees [rpart(), plotcp(), prune()]
library(rpart.plot) # Decision trees plotting [rpart.plot()]
library(lightgbm) # Light GBM [lgb.train()]
library(xgboost) # XGBoost [xgb.DMatrix(), xgb.train()]
library(MLmetrics) # Custom metrics (F1 score for example)
library(tidyverse) # Data manipulation
library(janitor) #Tidying data
library(gbm) #For gradient boosting
library(kableExtra) #Clean Tables
library(e1071) #Support Vector Machines
library(class) #KNN
library(GGally)
library(skimr) #Summary Tables
library(ggpattern)
library(MASS)
library(caTools) #randomforest
library(randomForest) #randomforest
```

```{r functions}

#Defining crosstable to be used in our analysis
Crosstab = function(df, var1, var2){
  # df: dataframe containing both columns to cross
  # var1, var2: columns to cross together.
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

# Function for calculating prediction error.
pred.error<-function(pred,truth){
mean(pred!=truth)
}

# Functions for calculating Precision, Accuracy, Recall and F1_score
# All of these are summarized in error_metrics function, that is the only one that needs to be run with input table(True values,Predicted Values)

# Input variable for all functions is table(True values,Predicted Values)
# For F1_score we use sensitivity and precision derived from the functions

sensitivity = function(cm_table){
  cm_table = as.data.frame(cm_table)
  return(cm_table[4,'Freq']/(cm_table[4,'Freq'] + cm_table[2,'Freq']))
}

specificity = function(cm_table){
  cm_table = as.data.frame(cm_table)
  return(cm_table[1,'Freq']/(cm_table[1,'Freq'] + cm_table[3,'Freq']))
}


precision = function(cm_table){
  cm_table = as.data.frame(cm_table)
  return(cm_table[4,'Freq']/(cm_table[4,'Freq'] + cm_table[3,'Freq']))
}

accuracy = function(cm_table){
  cm_table = as.data.frame(cm_table)
  return((cm_table[1,'Freq'] + cm_table[4,'Freq'])/(cm_table[1,'Freq'] + cm_table[2,'Freq'] + cm_table[3,'Freq'] + cm_table[4,'Freq']))
}

f1_score = function(precision,sensitivity){
  return((2*sensitivity*precision)/(sensitivity + precision))
}

# Just run the error metrics function to get all the values for metrics
error_metrics = function(cm_table){
  s = sensitivity(cm_table)
  sp = specificity(cm_table)
  p = precision(cm_table)
  a = accuracy(cm_table)
  f1 = f1_score(s,p)
  
  temp = data.frame(rbind(round(c(s,sp,p,a,f1),4)*100))
  colnames(temp) = c("Sensitivity","Specificity","Precision","Accuracy","F1 score")
  return(temp)
}

```

\newpage

# Introduction

A Portuguese banking institution launched directed marketing campaign to promote their products. These marketing campaigns were based on telephonic calls. It is important for the institution to know whether the campaign is effective in converting clients, thus data was collected about the subscription of the product by the clients contacted. In this report, we aim to apply different classification techniques to the data gathered to predict the success of this bank marketing campaign. The performance of the different classification techniques, like Support Vector Machines, Decision Trees, K nearest neighbors and so on, will be compared against each other to determine the model that produces the best results. This analysis can help banks optimize their marketing campaigns by targeting customers that are more likely to subscribe to the term deposit.

# Description of the Dataset

The dataset under study relates to 17 campaigns that occurred between May 2008 and November 2010. 

```{r dataset}

#Loading dataset
data = read.csv("group_14.csv")
colnames_desc <- c("age at the contact date (Numeric)",
                   "type of job (11 Categories)",
                   "marital status (3 categories)",
                   "education level (7 Categories)",
                   "has credit in default? ('no', 'yes')",
                   " has housing loan? ('no', 'yes')",
                   " has personal loan? ('no', 'yes')",
                   "contact communication type (‘cellular’, ‘telephone’)",
                   " last contact month of year (10 Categories)",
                   "last contact day of the week (5 Categories)",
                   "last contact duration, in seconds (Numeric)",
                   "number of contacts performed during this campaign and for this client (Numeric)",
                   "number of days that passed by after the client was last contacted from a previous campaign",
                   "number of contacts performed before this campaign and for this client (Numeric)",
                   "outcome of the previous marketing campaign (3 Categories)",
                   "employment variation rate – quarterly indicator (Numeric)",
                   "consumer price index – monthly indicator (Numeric)",
                   "consumer confidence index – monthly indicator (Numeric)",
                   "euribor (Euro Interbank Offered Rate) 3 month rate – daily indicator (Numeric)",
                   "number of employees – quarterly indicator (Numeric)",
                   "has the client subscribed a term deposit? ('no','yes')"
)
dataset_desc <- cbind(colnames(data), colnames_desc)
colnames(dataset_desc) <- c("Variable Name", "Description")

kable(dataset_desc, caption = '\\label{tab:data} Description of the Bank Marketing Dataset') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

```

Table \ref{tab:data} gives the details about each variable in our dataset including the description, type of variable and the number of categories present. Note that that there are missing values for some variables which will be dealt with in the further sections and have not been mentioned in the table above. The target variable in this dataset is whether or not the customer responded positively to the bank's marketing campaign. This is indicated by the binary variable "y". 

*It is important to note that the original dataset gathered by the Portuguese researchers contains over 40,000 observations, however this analysis is based on a subset of the complete dataset containing randomly selected 10,000 observations. Thus, the analysis in the report has less predictive power and accuracy.*


# Exploratory Data Analysis

Descriptive Analysis has been performed to understand the overall structure and features of the dataset. 

## Data Cleaning

The first step is to the clean the data by looking into missing values and any anomalies in the dataset. Table \ref{tab:miss} shows the number of missing values in the data for each variable and the percentage of the number of the missing values out of the total observations. 

```{r}
#Calculating the number of missing values

data=replace(data,data=='',"missing")
data %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "Variable", value = "no_missing") %>% 
  mutate("perc(%)" = round(no_missing / dim(data)[1],4)*100) %>% 
  filter(no_missing != 0) %>% 
  arrange(-no_missing) %>% 
  kable(caption = '\\label{tab:miss} Number of Missing values') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

```


```{r missing values, include = FALSE}
#Treating missing values

#Education 
Crosstab(data,"education","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

#Job
Crosstab(data,"job","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

#Marital status
Crosstab(data,"marital","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data%>%
  filter(education!="missing", job!="missing",marital!="missing",housing!="missing" )

```

For the variables 'Education', 'Housing', 'Loan', 'Job' and 'Marital', the number of missing values are less than 5% of the total observations. The proportion of the response variables for these values is the same as the data as a whole. These observations are not expected to be very influential in the classification. Thus, these observations have been removed from the dataset.

```{r cleaning}
#Default variable
Default_table <- as.data.frame(table(data2$default)) %>% 
  transmute(Value = Var1, Frequency = Freq) %>% 
  mutate( 'perc' = round(Frequency/ sum(Frequency),4)*100) 

Default_table %>% 
  kable(caption = '\\label{tab:default} Split of the Default Variable') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

#As we can see from the table, only 1 individual replied with "yes", that they have credit in default. 
#78.5% individuals answered "no" and 21.5% did not reply at all. Hence, this variable is of not much significance as we get no information from it. Thus, we would remove this from our dataset.

data2=data2 %>%
  dplyr::select(-default)

```

Additionally, after the above adjustment, a few variables have been analysed to check their influence on the response variable. 

* Table \ref{tab:default} shows the split of the 'Default' variable. It can be seen that `r Default_table$perc[2]`% individuals answered "no" and `r Default_table$perc[1]`% did not reply at all. Hence, this variable is not of much significance.

```{r, include = FALSE}
#Checking missing variables again
data2 %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "variable", value = "no_missing") %>% 
  arrange(-no_missing)

#No missing values left in our final dataset
```

```{r, include = FALSE}
#Checking Significance of some variables
#Housing
Crosstab(data,"housing","y")
chisq.test(data$housing,data$y)
#Since, the chi-square test for this variable has p-value less than 0.05 there is a significant association with the response variable. Hence, we would keep this in our dataset.

#Loan
Crosstab(data,"loan","y")
chisq.test(data$loan,data$y)
#Since, the chi-square test for this variable has p-value more than 0.05 there is no significant association with the response variable. Hence, we would exclude this from our dataset.

data2= data2 %>%
  dplyr::select(-loan)

data2 = data2 %>% 
  dplyr::select(-duration)
```

* A chi square test performed on the variable 'Loan' resulted in the p value of `r round(chisq.test(data$loan,data$y)$p.value,2)*100`%. Thus, there is not significant relationship of this variable with the response variable. 
* As our goal is to determine whether the client will subscribe to the term deposit or not, it  is difficult to know the duration of the call before hand. Thus, this has little influence on the response variable

Based on the above, the variables 'Default', 'Loan' and 'Duration' have been removed from the dataset.

```{r data split, include = FALSE}
#Train-test split.Using 80%/20% split.
set.seed(123)

n=nrow(data2)
ind=sample(c(1:n),0.8*n)
data.train=data2[ind,]
data.test=data2[-ind,]
```


## Data Splitting

The cleaned data contains `r dim(data2)[1]` observations. This is split into training and test datasets for the modelling and testing purposes. The training contains `r dim(data.train)[1]` observations (80% of the cleaned data) and the test data contains `r dim(data.test)[1]` observations (20% of the cleaned data).

The exploratory data analysis has been performed on the Training dataset.

## Exploratory Analysis of Categorical Variables

Table \ref{tab:summ2} shows the summary of the categorical variables. The table shows the number of unique categories for each variable. It also shows that there are no remaining missing values.

```{r summary cat}

#Creating summary statistics for categorical variables
data.train %>% 
  dplyr::select(job, marital, education, housing, contact, month, day_of_week, poutcome, y) %>% 
  skim() %>% 
  transmute(Variable = skim_variable, Missing = n_missing, Complete_Rate = complete_rate,
            Unique= character.n_unique) %>%
  kable(caption =
          '\\label{tab:summ2} Summary Statistics of Categorical Variables',
        booktabs = TRUE, format = "latex", digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")
```

The frequency plots and the barplots showing the proportion of the 2 categories of the response variable have been analysed (*all variables have not been displayed*). These can be seen in figure \ref{fig:bar}. Some inferences from the plots are given below:

* Variable 'Job'- The proportion of 'retired' individuals resulting in 'yes' is higher than the other categories
* Variable 'Education' - 'university degree' and 'professional course' have a higher proportion of resulting in 'yes'
* Variable 'Contact'- there have been more term deposits from cellular responders, 14.5% as compared to telephone responders which is just 5.6% 
* Variable 'marital'- the proportion that resulted in 'yes' is not marked differently across marital status

```{r, fig.show="hold", out.width = '50%', fig.cap = "\\label{fig:bar} Barplots of Selected Categorical Variables"}

data.train=data.train %>%
  mutate(month=factor(data.train$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))
data.train=data.train %>%
  mutate(day_of_week=factor(data.train$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

#Job
data.train %>% 
  ggplot(aes(y = job, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Job')

data.train %>% ggplot(aes(y = job, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Job')

#the high proportion of 'retired' job category resulting in 'yes' 

#Contact
data.train %>% 
  ggplot(aes(y = contact, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Contact')

data.train %>% ggplot(aes(y = contact, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Contact')
 #Here, we can see that there have been more term deposits from cellular responders, 14.5% as compared to telephone responders which is just 5.6%. 

#Marital
data.train %>% 
  ggplot(aes(y = marital, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Marital Status')

data.train %>% ggplot(aes(y = marital, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Marital Status')
#the proportion that resulted in 'yes' is not marked differently across marital status. 

```

```{r, include = FALSE}


#Education
data.train %>% 
  ggplot(aes(y = education, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Education')

data.train %>% ggplot(aes(y = education, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Education')
#the proportion related to illiterate category is misleading because the overall sample size is very small 
#other than that, 'university degree' and 'professional course' have a higher proportion 

#Month
data.train %>% 
  ggplot(aes(y = month, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Month')

data.train %>% ggplot(aes(y = month, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Month')

#Housing
data.train %>% 
  ggplot(aes(y = housing, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Housing')

data.train %>% ggplot(aes(y = housing, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Housing')

#Day_of_week
data.train %>% 
  ggplot(aes(y = day_of_week, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Day of the week')

data.train %>% ggplot(aes(y = day_of_week, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Day of the week')
```

## Exploratory Analysis of Numeric Variables

Table \ref{tab: Summary Statistics} shows the summary statistics of the numeric variables. 

```{r}
#Creating summary statistics for numeric variables
my_skim <- skim_with(numeric = sfl(hist = NULL), 
                  base = sfl(n = length))
data.train %>% 
  dplyr::select(age,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed, campaign) %>% 
  my_skim() %>% 
  transmute(Variable=skim_variable, n = n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(format.args = list(big.mark = ","),
  caption = '\\label{tab: Summary Statistics} 
        Summary statistics for numerical variables', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```

The figure \ref{fig:pairs} shows the pairs plot of all numeric variables along with the categorical response variable 'y'. Some inferences from the plots are given below:

* There is high correlation between some variables. 'emp.var.rate' has a very strong correlation with multiple variables. Correlation of `r round(cor(data.train$nr.employed, data.train$emp.var.rate),4)*100`% with 'nr.employed', correlation of `r round(cor(data.train$euribor3m, data.train$emp.var.rate),4)*100`% and correlation of `r round(cor(data.train$nr.employed, data.train$cons.price.idx),4)*100`% with 'cons.price.idx'.

```{r, out.width = '100%', fig.align = "center", fig.cap = "\\label{fig:pairs} Correlation plots for numerical variables", fig.pos = "H"}

#Pairs plot of all numeric variables
num_variables = c('age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')
pairs_plot <- c('age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','y')
ggpairs(data.train,columns = pairs_plot,  axisLabels = "none",
        columnLabels = gsub('.', ' ', pairs_plot, fixed = T),
        labeller = label_wrap_gen(10))+
   theme_bw(base_size = 10)
```

* The boxplots for 'emp.var.rate' and 'euribor3m' show some inherent difference between the categories. Figure \ref{fig:dens} shows the density plots for these variables which show a difference in the density of the 2 categories.
* For the variable 'pdays' 999 means that the client has not been contacted before, this constitues majority of the clients. In the training data `r sum(data.train$pdays[]==999)` observations have the values 999 for the variable 'pdays'. Thus, this variable has been converted into a binary variable 'n_contact' which defines whether the individual was previously contacted or not. Additionally, a chi-squared test indicates that when a client is contacted before, they are more likely to say 'yes'.
* The 'campaign' variable has been converted into a binary variable with the categories 'less than 15' and 'more than 15' considering the split of the data.

```{r, figures-side, fig.show="hold", out.width = '50%', fig.cap = "\\label{fig:dens} Density Plots of 'euribor3m' and 'emp.var.rate"}
#the density plot for the two variables
data.train %>% ggplot(aes(x = euribor3m)) +
  geom_density(aes(linetype = y))

data.train %>% ggplot(aes(x = emp.var.rate)) +
  geom_density(aes(linetype = y))
```


```{r, include = FALSE}
# Pdays : Number of days passed after client was last contacted by a previous campaign

data.train = data.train %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              dplyr::select(-pdays)

data.test = data.test %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              dplyr::select(-pdays)

Crosstab(data.train,'n_contact','y')
chisq.test(data.train$n_contact,data.train$y)
# It seems that when a client is contacted before,they are more receptive towards saying Yes
# This is verified with the chisq test as well.
```

```{r, include=FALSE}
#Transforming the 'campaign' variable
data.train = data.train %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

data.test = data.test %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

```

## Data Scaling

The data being used for further processing and model building includes the following variables after the cleaning and some data transformation performed in the above sections:

1. Categorical Variables: job, marital, education, housing, contact, month, day_of_week, campaign, poutcome, n_contact, y
2. Numerical Variables: age, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed
*y is the binary response variable for the study*

It is important to note that the numerical variables in the data are measured at different scales and thus do not contribute equally to the model fitting. This might create a bias. Thus, the numerical variables have been scaled using min-max normalization. The mathematical formulation of this is:

$$ x_{scaled} = \frac{x-min(x)}{max(x)- min(x)}$$ 


```{r Train data, include= FALSE}
data.train.num <- data.train %>%
  dplyr::select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.train.char <- data.train %>%
  dplyr::select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

min = sapply(data.train.num,min)
range = sapply(data.train.num, range)

data.train.num = (data.train.num-min)/ range

data.train.char <- data.train.char %>%
  map_dfc(as.factor)

data.train.scaled <- cbind(data.train %>% 
                             dplyr::select(age, month, day_of_week, previous), data.train.char, data.train.num)

data.train.scaled <- data.train.scaled %>% 
  dplyr::select(order(colnames(data.train.scaled)))
```

```{r Test data, include= FALSE}
data.test=data.test %>%
  mutate(month=factor(data.test$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))
data.test=data.test %>%
  mutate(day_of_week=factor(data.test$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

data.test.num <- data.test %>%
  dplyr::select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.test.char <- data.test %>%
  dplyr::select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

data.test.num = (data.test.num-min)/ range

data.test.char <- data.test.char %>%
  map_dfc(as.factor)

data.test.scaled <- cbind(data.test %>% 
                            dplyr::select(age, month, day_of_week,previous), data.test.char, data.test.num)

data.test.scaled <- data.test.scaled %>% 
  dplyr::select(order(colnames(data.test.scaled)))

```

# Statistical Modelling

This section covers the classification techniques used to predict the success of the marketing campaign.

## Model 1: Support Vector Machines

Support vector machine is a classification technique used for a binary output variable. In its basic form, SVM is a linear classifier which fits a hyperplane to the data that divides the two classes of the binary output into separate regions. Any new point in either region is classified as such. 

We can use kernels to use non-linear classification with SVM. 

If a linear classification is not possible, or does not give particularly good predictions, we can transform the input variables into a higher dimensional vector space which can be classified using a linear hyperplane in the higher dimensional space and then project the result back onto our original vector space. Projecting the decision boundary in a lower dimension vector space will lead to a non-linear classifier in the original vector space. We can make the calculations easier by transforming the variables with known functions which are called kernels. 

Commonly used kernels are: 

* Linear 
* Polynomial 
* Radial  

All of the above 3 kernels have been considered in this analysis. 

```{r, include= FALSE}
#Support Vector Machines
# Values for hyperparameters
set.seed(123)
cost_range <- c(0.01,0.1,1,2,5,10)
degree_range <- 2:4
gamma_range <- c(0.001,0.01,0.1,1,10)

# Since the tune.svm() function is too computationally expensive, we will split our training dataset again into training and validation set to tune hyperparameters for different kernels

# Index for splitting data
n = nrow(data.train.scaled)
sub_idx = sample(c(1:n),n*0.15)
```

So, the training dataset was split into a training set and a validation set for hyperparameter tuning. After running the model for the 3 kernels and different values for the hyper parameter, the model with minimum error was chosen as well, also considering accuracy and sensitivity to be highest. 

```{r, include = FALSE}
# SVM with linear kernel:

C.error <- numeric(length(cost_range))

for(i in 1:length(cost_range)){
  model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="linear",cost =cost_range[i])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i] <- pred.error(pred.model,data.train.scaled$y)
}

cost_selected = cost_range[which.min(C.error)]
optim.svm.linear = svm(y~.,data = data.train.scaled[-sub_idx], type="C-classification", kernel="linear",cost = cost_selected)

data.train.pred = predict(optim.svm.linear,data.train.scaled[-sub_idx,])
data.test.pred = predict(optim.svm.linear,data.test.scaled)
print("Table for linear SVM training data:")
result.svm.linear.train <- error_metrics(table(data.train.scaled[-sub_idx,]$y,data.train.pred))
result.svm.linear.test <- error_metrics(table(data.test.scaled$y, data.test.pred))

data.train.pred = predict(optim.svm.linear,data.train.scaled[sub_idx,])
print("Table for linear SVM Validation data:")
table(data.train.scaled[sub_idx,]$y,data.train.pred)

# SVM with polynomial kernel
C.error = matrix(nrow = length(degree_range),ncol = length(cost_range))
for(i in 1:length(degree_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

degree_selected = degree_range[which(C.error == min(C.error), arr.ind=TRUE)[,'row']]
cost_selected = cost_range[which(C.error == min(C.error), arr.ind=TRUE)[,'col']]
optim.svm.poly = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.poly,data.train.scaled[-sub_idx,])
data.test.pred = predict(optim.svm.poly,data.test.scaled)
print("Table for poly SVM training data")
result.svm.poly.train <- error_metrics(table(data.train.scaled[-sub_idx,]$y,data.train.pred))
result.svm.poly.test <- error_metrics(table(data.test.scaled$y, data.test.pred))

data.train.pred = predict(optim.svm.poly,data.train.scaled[sub_idx,])
print("Table for poly SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# SVM with Radial kernel

C.error = matrix(nrow = length(gamma_range),ncol = length(cost_range))


for(i in 1:length(gamma_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

gamma_selected = gamma_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'row'])]
cost_selected = cost_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'col'])]
optim.svm.radial = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.radial,data.train.scaled[-sub_idx,])
data.test.pred = predict(optim.svm.radial,data.test.scaled)
print("Table for radial SVM training data")
result.svm.radial.train <- error_metrics(table(data.train.scaled[-sub_idx,]$y,data.train.pred))
result.svm.radial.test <- error_metrics(table(data.test.scaled$y, data.test.pred))

data.train.pred = predict(optim.svm.radial,data.train.scaled[sub_idx,])
print("Table for radial SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# The linear and polynomial SVM perform much better than the radial kernel, the radial kernel either overfits the data or classifies all values as 'No' so even with good accuracy , it is not at all a good fit for the data.
```

```{r SVM results}
#Comparing the three methods
rbind(result.svm.linear.train,
result.svm.poly.train, result.svm.radial.train) %>% 
  mutate(Method = c("Linear","Polynomial", "Radial")) %>% 
  relocate('Method') %>% 
  kable(caption = '\\label{tab: SVM} 
        Results for Support Vector Machines (in percentage)', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```
*These results are based on training data*

From the table \ref{tab: SVM}, the ‘linear’ kernel with the cost parameter set to ‘0.01’ turned out to gives us the best result for the SVM classifier. 

Even though the accuracy of the model is 90%, the sensitivity is low. The reason could be an unbalanced dataset. However, a different model might be a better fit for the given data. 

## Model 2: K-Nearest Neighbours

Under k-nearest neighbours classification technique, the k-nearest labelled points predict the class of this point to be the class that most of its neighbours share. 

Here, the value of k has been chosen through Cross-validation approach. Cross validation has been done in two different ways - K-fold cross validation and leave-one-out cross validation. In K- fold cross validation, the data is divided into K roughly equal sized parts. Firstly, the validation data is taken as the first set and training data as all the other sets and the validation error rate/classification rate is estimated for this split. The process is then repeated K-1 more times, with a different part of the data set as the validation data. The final error rate is the average of the K error rates estimated. 

Leave-one-out cross validation is performed entirely on the training data. 

```{r kNN Technique, include = FALSE}

#Preparing the data for kNN
data.kNN.char = data.train.scaled %>%
  dplyr::select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char = data.kNN.char %>%
  map_dfc(as.numeric)

data.kNN <- cbind(data.train.scaled %>% 
                    dplyr::select(age,previous), data.kNN.char, data.train.num)

data.kNN = data.kNN %>%
  dplyr::select(order(colnames(data.kNN)))

N=nrow(data.kNN)
div=sample(c(1:N),0.8*N)
data.train.kNN=data.kNN[div,]
data.val.kNN=data.kNN[-div,]

classes = data.train.kNN[,18]
classes.val=data.val.kNN[,18]

#For test data
data.kNN.char.test = data.test.scaled %>%
  dplyr::select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char.test = data.kNN.char.test %>%
  map_dfc(as.numeric)

data.test.kNN <- cbind(data.test.scaled %>% 
                         dplyr::select(age,previous), data.kNN.char.test, data.test.num)

data.test.kNN = data.test.kNN %>%
  dplyr::select(order(colnames(data.test.kNN)))
```

```{r, include = FALSE}
#K-fold cross validation, to find the right value for k
K=c(1:15)
val.error=c()
for (k in K) {
  val.pred=knn(data.train.kNN, data.val.kNN, classes, k=k)
  val.error[k]=mean(classes.val != val.pred)
}
k.opt <- which.min(val.error)

#Predicting the values using train data
train.pred=knn(data.train.kNN, data.val.kNN, classes, k=k.opt)
train.tab=table(data.val.kNN[,18],train.pred)
train.rate=prop.table(train.tab,margin=1)

#Leave one-out-cross validation

K <- c(1:15)
cv.error <- c()
for (k1 in K){
  train.pred <- knn.cv(data.kNN, data.kNN[,18], k=k1)
  cv.error[k1] <- mean(data.kNN[,18] != train.pred)
}

k.opt2=which.min(cv.error)

train.pred2 <- knn(data.kNN, data.train.kNN, data.kNN[,18], k=k.opt2)
train.tab2=table(data.train.kNN[,18],train.pred2)
train.rate2=prop.table(train.tab2,margin=1)

#Predicting for test data
test.pred2 <- knn(data.kNN, data.test.kNN, data.kNN[,18], k=k.opt2)
test.tab2=table(data.test.kNN[,18],test.pred2)
test.rate2=prop.table(test.tab2,margin=1)

```

```{r, fig.show="hold", out.width = '50%', fig.cap = "\\label{fig:KNN} Validation error rate for KNN Models"}
plot(K, val.error, type="b", ylab="validation error rate", main = "K-fold cross validation")
plot(K, cv.error, type="b", ylab="validation error rate", main = "Leave one-out-cross validation")
```

```{r KNN results}
#Comparing the two cross-validations
rbind(error_metrics(train.tab),
error_metrics(train.tab2)) %>% 
  mutate(Method = c("K-Fold","Leave-one-out")) %>% 
  relocate('Method') %>% 
  kable(caption = '\\label{tab: KNN} 
        Results for K-nearest neighbours (in percentage)', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

result.knn.train <- error_metrics(train.tab2)
result.knn.test <- error_metrics(test.tab2)

#Out of the two cross-validations, both the sensitivity and accuracy are better in leave one out cross validation. 
```
*These results are based on training data*

From the figure \ref{fig:KNN}, the optimum k value from both the methods is coming out to be 6, however the Accuracy and Sensitivity rates on the training data, as seen in table \ref{tab: KNN}, are higher for leave-one-out cross validation. Thus, model 2 has been chosen. 

## Model 3: Decision Tree

Classification tree or Decision tree is a kind of partitioning method. The results of these trees are partitions of a set of possible values of explanatory variables.  

Under this method, we divide our explanatory variables or features into several disjoint and non-overlapping regions. There is a cut-off point chosen on each feature with the resulting split leading to a further split or a terminal node where a class is predicted.  Then, the class label of a given test observation is predicted as the most commonly occurring class of training observations in the region to which it belongs. 

The decision tree in this study is built by selecting the best parameters through tuning. 
 
```{r, include= FALSE}
#Tree Based Methods

tree <- rpart(y~ ., data = data.train.scaled, method = "class", minsplit = 4, minbucket = 2, cp = 0.04)
rpart.plot(tree, type = 2, extra = 106)

n= ncol(data.train.scaled)
data.train.rt.pruned <- prune(tree, cp=0.04)

train.pred <- predict(data.train.rt.pruned, newdata= data.train.scaled,type="class")
test.pred <- predict(data.train.rt.pruned, newdata= data.test.scaled,type="class")

result.tree.train <- error_metrics(table(data.train.scaled[,n], train.pred))
result.tree.test <- error_metrics(table(data.test.scaled[,n], test.pred))

```

```{r,  fig.pos = "H", out.width = '75%', fig.cap = "\\label{fig:tree} Decision Tree", fig.align = "center"}
rpart.plot(data.train.rt.pruned)
```

Figure \ref{fig:tree} shows the decision tree selected for this data. The decision tree results in a sensitivity of `r result.tree.train[1]`% and accuracy of `r result.tree.train[4]`% on the training data.

## Model 4: Random Forest

Random forest is a technique that is based on Decision trees algorithm. It builds a forest of decision trees through bagging that helps improve the accuracy.  Random forest adds some randomness to the model, by only considering a random subset of all the features for splitting a node. It builds smaller trees using these subsets and then makes predictions by taking an average across all the models to generate a low variance model.  

Random forests are an improved version of Decision trees, as they solve the issue of overfitting and give more accurate results. 

```{r Random Forest, include=FALSE}
#Random Forest
set.seed(123)
classifier_RF = randomForest(x = data.train.scaled %>% 
                               dplyr::select(-y),
                             y = data.train.scaled$y,
                             ntree = 500)


pred.train.rf = predict(classifier_RF, newdata = data.train.scaled[-n])
pred.test.rf = predict(classifier_RF, newdata = data.test.scaled %>% dplyr::select(-y))

result.rf.train <- error_metrics(table(data.train.scaled$y, classifier_RF$predicted))
result.rf.test <- error_metrics(table(data.test.scaled$y, pred.test.rf))
```

Figure \ref{fig:tree} shows the decision tree selected for this data. The random forest model results in a sensitivity of `r result.rf.train[1]`% and accuracy of `r result.rf.train[4]`% on the training data.

## Model 5: Gradient Boosting

Boosting algorithm is a method used to create an ensemble of simple individual models that together create a better model. First, an initial model is fitted to the data. Then a second model is built that focuses on correctly predicting the cases that were incorrectly predicted by the first model. The combination of these two models is better than either of the two models alone. This process of boosting is repeated, with each successive model attempting to correct for the shortcomings of the combined boosted ensemble of all the previous models.  

Gradient Boosting is a classification technique that uses this boosting algorithm. The word gradient is used because here the target outcomes for each case are set, based on the gradient (partial derivative of our loss function) of the error with respect to the prediction. This means that the target outcome for each case depends on how much changing that case’s prediction impacts the prediction error. The gradient can be used to find the direction in which to change the model parameters to reduce the error in the next round of training.  

When the target variable is continuous, Gradient Boosting Regressor is used and when it is a classification problem, Gradient Boosting Classifier is used. Since our target column is binary, Gradient Boosting Classifier has been used.  

Here, log likelihood is used as the loss function. When this function is differentiated, log(odds) is produced which is then used to find a value for which the loss function is minimum. This minimum value is the first prediction of our base model. Next, the pseudo residuals are calculated which are then used as target variables for our second model. Finally, new predictions are produced by adding this model to the base model. 

For this study, XGBoost has been used, which stands for Extreme Gradient Boosting. It is a specific implementation of the Gradient Boosting method which used more accurate approximations to find the best tree model. It computes second order gradients that provides more information about the direction of the gradients and how to get the minimum of our loss function. 

```{r Gradient boosting technique, include= FALSE}

#Fitting the model on the training set
set.seed(123)
model.gbm=train(y~.,data=data.train.scaled,method="xgbTree", trControl=trainControl("cv",number=10))

pred.class=model.gbm %>%
  predict(data.train.scaled)

#Compute model prediction accuracy rate
mean(pred.class==data.train.scaled[,18])
tab.gbm=table(pred.class,data.train.scaled[,18])
rate.gbm=prop.table(tab.gbm,margin = 1)

#Predicting on test data
pred.class.test=model.gbm %>%
  predict(data.test.scaled)

mean(pred.class.test==data.test.scaled[,18])
tab.gbm.test=table(pred.class.test,data.test.scaled[,18])
rate.gbm=prop.table(tab.gbm.test,margin = 1)

#Results
result.gbm.train <- error_metrics(tab.gbm)
result.gbm.test <- error_metrics(tab.gbm.test)

```

The sensitivity for this model is `r result.gbm.train[1]`% and the accuracy is almost `r result.gbm.train[4]`% on the training data. Overall, this appears to be a good fit to the data. 

## Model 6: Linear Discriminant Analysis

Linear Discriminant Analysis (LDA), a multi-class classification technique, does not directly compute the probabilities of the classes. It uses prior probabilities of the classes and bayes theorem to compute the class probabilities. For each class of the response, it assumes that the covariates are multivariate-gaussian distributed, with class specific mean vector and a variance-covariance matrix that is common across all classes. It computes a linear decision boundary using the bayes theorem and assigns the classes based on this boundary. 

```{r LDA, include = FALSE}
#Linear Discriminant Analysis
#data preparation to apply PCA to continuous correlated variables
data.train.num <- data.train.scaled %>% 
  dplyr::select(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed)
data.train.num.pca <- princomp(data.train.num, cor = T)

#the first component accounts for 99.8% variation

#atleast 80% variance to be retained.
#using cumulative proportion values to choose the components
data.train.scaledLda <- data.train.scaled %>% bind_cols(data.train.num.pca$scores[, 1:3]) %>% 
  dplyr::select(-c(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed))

data.test.num <- data.test.scaled %>% 
  dplyr::select(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed)
data.test.num.pca <- princomp(data.test.num, cor = T)
data.test.scaledlda <- data.test.scaled %>% 
  bind_cols(data.test.num.pca$scores[, 1:3]) %>% 
  dplyr::select(-cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed)
```

```{r , include = FALSE}
#start with a simpler model
#include the variables that deemed most imp. during exploratory analysis
lda.fit1 <- lda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)

```

```{r, include = FALSE}
#dataframe containing actual, predicted values and decision scores
predict.data <- data.train.scaledLda %>% dplyr::select(y)
predict.data['y_lda1'] <- predict(lda.fit1)$class
predict.data['y_lda1_probs'] <- predict(lda.fit1)$posterior[, 'yes']

#Test data
predict.data.test <- data.test.scaledlda %>% dplyr::select(y)
predict.data.test['y_lda1'] <- predict(lda.fit1, data.test.scaledlda)$class

#Results
result.lda.train <- error_metrics(table(predict.data$y,predict.data$y_lda1))
result.lda.test <- error_metrics(table(predict.data.test$y,predict.data.test$y_lda1))

```

```{r autoeval, eval=FALSE}
#auto_eval() will add variables to the model one by one and compare the model's
#sensitivity and precision. If any added variable gives improvement of over 1% point,
#it will display the formula for that model. It does not stop iteration if error
#occurs during model fitting
auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  lda1_sensi <- 0.21
  lda1_preci <- 0.656
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(lda.fit.trial <- lda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > lda1_sensi + 0.1 | preci > lda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```

Initially a model was fit using the most influential variables. After this model, a trial and error approach was taken to select the variables to add to the model. None of the other variables provided better prediction with respect to all the metrics. Hence the optimal Linear discriminant model was found to be the first model. The sensitivity and accuracy for the optimal LDA model was estimated to be `r result.lda.train[1]`% and `r result.lda.train[4]`% respectively on the training data. A 10-fold cross-validation approach was undertaken to estimate the rates. 

## Model 7: Quadratic Discriminant Analysis

Quadratic Discriminant Analysis (QDA) assumes that the values of covariates within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector (same as LDA) but also uses class-specific covariance matrix. Again, based on the bayes theorem, it creates a quadratic decision boundary using which it assigns the classes of the response. Because of higher number of parameter estimation involved, this has a higher variance that LDA, and we have the bias-variance tradeoff when choosing between the models. 


```{r QDA}
#the initial QDA model uses the same variables as LDA
qda.fit1 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)
```

```{r predictdataqda, include = FALSE}
#data frame containing predicted, actual values and decision scores for qda
predict.data['y_qda1'] <- predict(qda.fit1)$class
predict.data['y_qda1_probs'] <- predict(qda.fit1)$posterior[, 'yes']

#Test data
predict.data.test['y_qda1'] <- predict(qda.fit1, data.test.scaledlda)$class
table(predict.data$y,predict.data$y_qda1)
#Results
result.qda.train <- error_metrics(table(predict.data$y,predict.data$y_qda1))
result.qda.test <- error_metrics(table(predict.data.test$y,predict.data.test$y_qda1))
```
Same trial and error process as LDA was adopted starting with the best model determined by LDA.

```{r, eval = FALSE}

auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  qda1_sensi <- 0.217
  qda1_preci <- 0.65
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(qda.fit.trial <- qda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > qda1_sensi + 0.1 | preci > qda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```

```{r qda.fit2, eval=FALSE}
#job variable detected by auto_eval
#job variable added
qda.fit2 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome + job, data = data.train.scaledLda)

```

```{r, eval=FALSE}
predict.data['y_qda2'] <- predict(qda.fit2)$class
predict.data['y_qda2_probs'] <- predict(qda.fit2)$posterior[, 'yes']

confusionMatrix(predict.data$y_qda2, predict.data$y, positive = 'yes', mode = 'everything')

#Sensitivity for qda.fit2 increased but precision declined; Precision recall trade-off
```

```{r CV , eval=FALSE}
#cross validation results for all three models i.e.
#lda.fit1, qda.fit1, qda.fit2

set.seed(169)
folds <- createFolds(data.train.scaledLda$y, k = 10)

lda1_cv_sensi <- vector(mode = 'numeric', length = 10)
lda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda1_cv_sensi <- vector(mode = 'numeric', 10)
qda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda2_cv_sensi <- vector('numeric', 10)
qda2_cv_preci <- vector('numeric', 10)

for(i in 1:length(folds)){
  train <- data.train.scaledLda[-folds[[i]], ]
  test <- data.train.scaledLda[folds[[i]], ]
  
  lda1 <- lda(formula(lda.fit1$call), data = train)
  qda1 <- qda(formula(qda.fit1$call), data = train)
  qda2 <- qda(formula(qda.fit2$call), data = train)
  
  lda1_cv_sensi[i] <- recall(predict(lda1, test)$class, test$y, relevant = 'yes')
  lda1_cv_preci[i] <- precision(predict(lda1, test)$class, test$y, relevant = 'yes')
  
  qda1_cv_sensi[i] <- recall(predict(qda1, test)$class, test$y, relevant = 'yes')
  qda1_cv_preci[i] <- precision(predict(qda1, test)$class, test$y, relevant = 'yes')
  
  qda2_cv_sensi[i] <- recall(predict(qda2, test)$class, test$y, relevant = 'yes')
  qda2_cv_preci[i] <- precision(predict(qda2, test)$class, test$y, relevant = 'yes')
              
}

metric.frame <- data.frame(avg_sensi = c(mean(lda1_cv_sensi), mean(qda1_cv_sensi), mean(qda2_cv_sensi)),
                           avg_preci = c(mean(lda1_cv_preci), mean(qda1_cv_preci), mean(qda2_cv_preci)),
                           row.names = c('lda1', 'qda1', 'qda2'))

metric.frame
```

A 10-fold cross validation was employed to estimate the sensitivity and precision of this model. Using that, the sensitivity and accuracy were estimated to be `r result.qda.train[1]`% and `r result.qda.train[4]`% respectively on the training data. Not much improvement was seen when compared to LDA model, which is computationally less expensive to train. 

# Results

The results of all the selected models have been collated in the table \ref{tab: Results} below. 

```{r Results Table}
#Comparing the three methods
rbind(result.svm.linear.test, result.svm.poly.test, result.knn.test, result.tree.test, result.rf.test,
      result.gbm.test, result.lda.test, result.qda.test) %>% 
  mutate(Method = c("Support Vector Machines- Linear","Support Vector Machines- Polynomial", 
                    "K-Nearest Neighbors", "Decision Tree", "Random Forest", "Gradient Boosting", "Linear Discriminant Analysis",
                    "Quadratic Discriminant Analysis")) %>% 
  relocate('Method') %>% 
  kable(caption = '\\label{tab: Results} 
        Results for all Models (in percentage)', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```
*These results are based on Test Data*

For the selection of the best model, the most crucial rate to be considered in this case is the Sensitivity. Sensitivity gives the true positive rate i.e the probability of accurately predicting a 'yes'. In this case, the cost of falsely predicting a 'yes' would be more than falsely predicting a 'no' since the the banking institution would spend more on the marketing campaign if high success is predicted. 

Based on this selection criteria, Gradient Boosting is chosen since it has the highest sensitivity of `r result.gbm.test[1]`%.

# Conclusion

Different machine learning models were built to predict the success of the marketing campaigns undertaken by the Portuguese banking institution. 

The marketing campaign would be a success if a greater number of clients say yes to the term deposit, thus we need a model that identifies correctly maximum number of “yes”. Hence, sensitivity of a model would be the best selection criteria. Based on this, the model built using Gradient Boosting was chosen. This model has a maximum sensitivity rate of almost 64%, out of all the models. This means that the model manages to predict almost `r result.gbm.test[1]`% of the clients correctly who were ready to make the term deposit. This model’s accuracy is `r result.gbm.test[4]`%%, which is also quite high. 

Overall, our results show that the probability of clients saying ‘no’ is more than a ‘yes’. This indicates that the marketing campaign is not extremely effective.

\newpage 

# References

@Report1

@Report2

@Report3

@Website1

@Website2

@Website3

@Website4
