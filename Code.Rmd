---
title: "Predicting Effectiveness of Bank Marketing"
author: "Group 14: Mahima Mago, Ninad Khare, Shreyansi Jain, Subasish Behera"
output:
  pdf_document:
          latex_engine: xelatex
          number_sections: yes
          toc: yes
fig_caption: yes
---

```{r setup, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, eval = TRUE, include = TRUE, comment = NA, message = FALSE)
```

```{r libraries, include = FALSE}
#Loading the required packages

library(gmodels) # Cross Tables [CrossTable()]
library(caret) # ML [train(), confusionMatrix(), createDataPartition(), varImp(), trainControl()]
library(ROCR) # Model performance [performance(), prediction()]
library(plotROC) # ROC Curve with ggplot [geom_roc()]
library(pROC) # AUC computation [auc()]
library(PRROC) # AUPR computation [pr.curve()]
library(rpart) # Decision trees [rpart(), plotcp(), prune()]
library(rpart.plot) # Decision trees plotting [rpart.plot()]
library(ranger) # Optimized Random Forest [ranger()]
library(lightgbm) # Light GBM [lgb.train()]
library(xgboost) # XGBoost [xgb.DMatrix(), xgb.train()]
library(MLmetrics) # Custom metrics (F1 score for example)
library(tidyverse) # Data manipulation
library(janitor) #Tidying data
library(gbm) #For gradient boosting
library(kableExtra) #Clean Tables
library(e1071) #Support Vector Machines
library(class) #KNN
library(GGally)
library(skimr) #Summary Tables
library(ggpattern)
```

```{r functions}

#Defining crosstable to be used in our analysis
Crosstab = function(df, var1, var2){
  # df: dataframe containing both columns to cross
  # var1, var2: columns to cross together.
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

# Function for calculating prediction error.

pred.error<-function(pred,truth){
mean(pred!=truth)
}

```

\newpage

# Introduction

A Portuguese banking institution launched directed marketing campaign to promote their products. These marketing campaigns were based on telephonic calls. It is important for the institution to know whether the campaign is effective in converting clients, thus data was collected about the subscription of the product by the clients contacted. In this report, we aim to apply different classification techniques to the data gathered to predict the success of this bank marketing campaign. The performance of the different classification techniques, like Support Vector Machines, Decision Trees, K nearest neighbors and so on, will be compared against each other to determine the model that produces the best results. This analysis can help banks optimize their marketing campaigns by targeting customers that are more likely to subscribe to the term deposit.

# Description of the Dataset

The dataset under study relates to 17 campaigns that occurred between May 2008 and November 2010. 

```{r dataset}

#Loading dataset
data = read.csv("group_14.csv")
colnames_desc <- c("age at the contact date (Numeric)",
                   "type of job (11 Categories)",
                   "marital status (3 categories)",
                   "education level (7 Categories)",
                   "has credit in default? ('no', 'yes')",
                   " has housing loan? ('no', 'yes')",
                   " has personal loan? ('no', 'yes')",
                   "contact communication type (‘cellular’, ‘telephone’)",
                   " last contact month of year (10 Categories)",
                   "last contact day of the week (5 Categories)",
                   "last contact duration, in seconds (Numeric)",
                   "number of contacts performed during this campaign and for this client (Numeric)",
                   "number of days that passed by after the client was last contacted from a previous campaign",
                   "number of contacts performed before this campaign and for this client (Numeric)",
                   "outcome of the previous marketing campaign (3 Categories)",
                   "employment variation rate – quarterly indicator (Numeric)",
                   "consumer price index – monthly indicator (Numeric)",
                   "consumer confidence index – monthly indicator (Numeric)",
                   "euribor (Euro Interbank Offered Rate) 3 month rate – daily indicator (Numeric)",
                   "number of employees – quarterly indicator (Numeric)",
                   "has the client subscribed a term deposit? ('no','yes')"
)
dataset_desc <- cbind(colnames(data), colnames_desc)
colnames(dataset_desc) <- c("Variable Name", "Description")

kable(dataset_desc, caption = '\\label{tab:data} Description of the Bank Marketing Dataset') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

```

Table \ref{tab:data} gives the details about each variable in our dataset including the description, type of variable and the number of categories present. Note that that there are missing values for some variables which will be dealt with in the further sections and have not been mentioned in the table above. The target variable in this dataset is whether or not the customer responded positively to the bank's marketing campaign. This is indicated by the binary variable "y". 

*It is important to note that the original dataset gathered by the Portuguese researchers contains over 40,000 observations, however this analysis is based on a subset of the complete dataset containing randomly selected 10,000 observations. Thus, the analysis in the report has less predictive power and accuracy.*


# Exploratory Data Analysis

Descriptive Analysis has been performed to understand the overall structure and features of the dataset. 

## Data Cleaning

The first step is to the clean the data by looking into missing values and any anomalies in the dataset. Table \ref{tab:miss} shows the number of missing values in the data for each variable and the percentage of the number of the missing values out of the total observations. 

```{r}
#Calculating the number of missing values

data=replace(data,data=='',"missing")
data %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "Variable", value = "no_missing") %>% 
  mutate("perc(%)" = round(no_missing / dim(data)[1],4)*100) %>% 
  filter(no_missing != 0) %>% 
  arrange(-no_missing) %>% 
  kable(caption = '\\label{tab:miss} Number of Missing values') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

```


```{r missing values, include = FALSE}
#Treating missing values

#Education 
Crosstab(data,"education","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

#Job
Crosstab(data,"job","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

#Marital status
Crosstab(data,"marital","y")
#Removing missing values from the data, since less than 5% of the data has missing values for this field and the proportion of the response variable for these values is the same as the data at a whole. It is better to omit these values altogether.

data2=data%>%
  filter(education!="missing", job!="missing",marital!="missing",housing!="missing" )

```

For the variables 'Education', 'Housing', 'Loan', 'Job' and 'Marital', the number of missing values are less than 5% of the total observations. The proportion of the response variables for these values is the same as the data as a whole. These observations are not expected to be very influential in the classification. Thus, these observations have been removed from the dataset.

```{r cleaning}
#Default variable
Default_table <- as.data.frame(table(data2$default)) %>% 
  transmute(Value = Var1, Frequency = Freq) %>% 
  mutate( 'perc' = round(Frequency/ sum(Frequency),4)*100) 

Default_table %>% 
  kable(caption = '\\label{tab:default} Split of the Default Variable') %>% 
  kable_styling(font_size = 10, latex_options = "hold_position")

#As we can see from the table, only 1 individual replied with "yes", that they have credit in default. 
#78.5% individuals answered "no" and 21.5% did not reply at all. Hence, this variable is of not much significance as we get no information from it. Thus, we would remove this from our dataset.

data2=data2 %>%
  select(-default)

```

Additionally, after the above adjustment, a few variables have been analysed to check their influence on the response variable. 

* Table \ref{tab:default} shows the split of the 'Default' variable. It can be seen that `r Default_table$perc[2]`% individuals answered "no" and `r Default_table$perc[1]`% did not reply at all. Hence, this variable is not of much significance.

```{r, include = FALSE}
#Checking missing variables again
data2 %>% 
  summarise_all(list(~sum(. == "missing"))) %>% 
  gather(key = "variable", value = "no_missing") %>% 
  arrange(-no_missing)

#No missing values left in our final dataset
```

```{r, include = FALSE}
#Checking Significance of some variables
#Housing
Crosstab(data,"housing","y")
chisq.test(data$housing,data$y)
#Since, the chi-square test for this variable has p-value less than 0.05 there is a significant association with the response variable. Hence, we would keep this in our dataset.

#Loan
Crosstab(data,"loan","y")
chisq.test(data$loan,data$y)
#Since, the chi-square test for this variable has p-value more than 0.05 there is no significant association with the response variable. Hence, we would exclude this from our dataset.

data2= data2 %>%
  select(-loan)

data2 = data2 %>% 
  select(-duration)
```

* A chi square test performed on the variable 'Loan' resulted in the p value of `r round(chisq.test(data$loan,data$y)$p.value,2)*100`%. Thus, there is not significant relationship of this variable with the response variable. 
* As our goal to to determine whether the client will subscribe to the term deposit or not and it  is difficult to know the duration of the call before hand. Thus has little influence on the response variable

Based on the above, the variables 'Default', 'Loan' and 'Duration' have been removed from the dataset.

```{r data split, include = FALSE}
#Train-test split.Using 80%/20% split.
set.seed(123)

n=nrow(data2)
ind=sample(c(1:n),0.8*n)
data.train=data2[ind,]
data.test=data2[-ind,]
```


## Data Splitting

The cleaned data contains `r dim(data2)[1]` observations. This is split into training and test datasets for the modelling and testing purposes. The training contains `r dim(data.train)[1]` observations (80% of the cleaned data) and the test data contains `r dim(data.test)[1]` observations (20% of the cleaned data).

The exploratory data analysis has been performed on the Training dataset.

## Exploratory Analysis of Categorical Variables

Table \ref{tab:summ2} shows the summary of the categorical variables. The table shows the number of unique categories for each variable. It also shows that there are no remaining missing values.

```{r summary cat}

#Creating summary statistics for categorical variables
data.train %>% 
  select(job, marital, education, housing, contact, month, day_of_week, poutcome, y) %>% 
  skim() %>% 
  transmute(Variable = skim_variable, Missing = n_missing, Complete_Rate = complete_rate,
            Unique= character.n_unique) %>%
  kable(caption =
          '\\label{tab:summ2} Summary Statistics of Categorical Variables',
        booktabs = TRUE, format = "latex", digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")
```

The frequency plots and the barplots showing the proportion of the 2 categories of the response variable have been analysed (*all variables have not been displayed*). Some inferences from the plots are given below:

* Variable 'Job'- The proportion of 'retired' individuals resulting in 'yes' is higher than the other categories
* Variable 'Education' - 'university degree' and 'professional course' have a higher proportion of resulting in 'yes'
* Variable 'Contact'- there have been more term deposits from cellular responders, 14.5% as compared to telephone responders which is just 5.6% 
* Variable 'marital'- the proportion that resulted in 'yes' is not marked differently across marital status

```{r,include = FALSE, fig.show="hold", out.width = '45%', fig.cap = "\\label{fig:bar} Barplots of Selected Categorical Variables"}

data.train=data.train %>%
  mutate(month=factor(data.train$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))
data.train=data.train %>%
  mutate(day_of_week=factor(data.train$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

#Job
data.train %>% 
  ggplot(aes(y = job, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Job')

data.train %>% ggplot(aes(y = job, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Job')

#the high proportion of 'retired' job category resulting in 'yes' 

#Contact
data.train %>% 
  ggplot(aes(y = contact, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Contact')

data.train %>% ggplot(aes(y = contact, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Contact')
 #Here, we can see that there have been more term deposits from cellular responders, 14.5% as compared to telephone responders which is just 5.6%. 

```

```{r, include = FALSE}

#Marital
data.train %>% 
  ggplot(aes(y = marital, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Marital Status')

data.train %>% ggplot(aes(y = marital, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Marital Status')
#the proportion that resulted in 'yes' is not marked differently across marital status. 

#Education
data.train %>% 
  ggplot(aes(y = education, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Education')

data.train %>% ggplot(aes(y = education, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Education')
#the proportion related to illiterate category is misleading because the overall sample size is very small 
#other than that, 'university degree' and 'professional course' have a higher proportion 

#Month
data.train %>% 
  ggplot(aes(y = month, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Month')

data.train %>% ggplot(aes(y = month, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Month')

#Housing
data.train %>% 
  ggplot(aes(y = housing, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Housing')

data.train %>% ggplot(aes(y = housing, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Housing')

#Day_of_week
data.train %>% 
  ggplot(aes(y = day_of_week, fill = y, pattern = y)) +
  geom_bar_pattern(color = 'black')+
  scale_fill_grey()+
  labs(x = 'frequency', y = 'Day of the week')

data.train %>% ggplot(aes(y = day_of_week, pattern = y, fill = y))+ 
  geom_bar_pattern(color = 'black', position = 'fill') +
  scale_fill_grey()+
  labs(x = 'proportion', y = 'Day of the week')
```

## Exploratory Analysis of Numeric Variables

Table \ref{tab: Summary Statistics} shows the summary statistics of the numeric variables. 

```{r}
#Creating summary statistics for numeric variables
my_skim <- skim_with(numeric = sfl(hist = NULL), 
                  base = sfl(n = length))
data.train %>% 
  select(age,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed, campaign) %>% 
  my_skim() %>% 
  transmute(Variable=skim_variable, n = n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(format.args = list(big.mark = ","),
  caption = '\\label{tab: Summary Statistics} 
        Summary statistics for numerical variables', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```

The figure \ref{fig:pairs} shows the pairs plot of all numeric variables along with the categorical response variable 'y'. Some inferences from the plots are given below:

* There is high correlation between some variables. 'emp.var.rate' has a very strong correlation with multiple variables. Correlation of `r round(cor(data.train$nr.employed, data.train$emp.var.rate),4)*100`% with 'nr.employed', correlation of `r round(cor(data.train$euribor3m, data.train$emp.var.rate),4)*100`% and correlation of `r round(cor(data.train$nr.employed, data.train$cons.price.idx),4)*100`% with 'cons.price.idx'.

```{r, out.width = '100%', fig.align = "center", fig.cap = "\\label{fig:pairs} Correlation plots for numerical variables", fig.pos = "H"}

#Pairs plot of all numeric variables
num_variables = c('age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed')
pairs_plot <- c('age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','y')
ggpairs(data.train,columns = pairs_plot,  axisLabels = "none",
        columnLabels = gsub('.', ' ', pairs_plot, fixed = T),
        labeller = label_wrap_gen(10))+
   theme_bw(base_size = 10)
```

* The boxplots for 'emp.var.rate' and 'euribor3m' show some inherent difference between the categories. Figure \ref{fig:dens} shows the density plots for these variables which show a difference in the density of the 2 categories.
* For the variable 'pdays' 999 means that the client has not been contacted before, this constitues majority of the clients. In the training data `r sum(data.train$pdays[]==999)` observations have the values 999 for the variable 'pdays'. Thus, this variable has been converted into a binary variable 'n_contact' which defines whether the individual was previously contacted or not. Additionally, a chi-squared test indicates that when a client is contacted before, they are more likely to say 'yes'.
* The 'campaign' variable has been converted into a binary variable with the categories 'less than 15' and 'more than 15' considering the split of the data.

```{r, figures-side, fig.show="hold", out.width = '50%', fig.cap = "\\label{fig:dens} Density Plots of 'euribor3m' and 'emp.var.rate"}
#the density plot for the two variables
data.train %>% ggplot(aes(x = euribor3m)) +
  geom_density(aes(linetype = y))

data.train %>% ggplot(aes(x = emp.var.rate)) +
  geom_density(aes(linetype = y))
```


```{r, include = FALSE}
# Pdays : Number of days passed after client was last contacted by a previous campaign

data.train = data.train %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              select(-pdays)

data.test = data.test %>%
              mutate(n_contact = if_else(pdays == 999,"no","yes")) %>%
              select(-pdays)

Crosstab(data.train,'n_contact','y')
chisq.test(data.train$n_contact,data.train$y)
# It seems that when a client is contacted before,they are more receptive towards saying Yes
# This is verified with the chisq test as well.
```

```{r, include=FALSE}
#Transforming the 'campaign' variable
data.train = data.train %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

data.test = data.test %>%
  mutate(campaign=ifelse(campaign<=15,"Less Than 15","More Than 15"))

```

## Data Scaling

The data being used for further processing and model building includes the following variables after the cleaning and some data transformation performed in the above sections:

1. Categorical Variables: job, marital, education, housing, contact, month, day_of_week, campaign, poutcome, n_contact, y
2. Numerical Variables: age, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed
*y is the binary response variable for the study*

It is important to note that the numerical variables in the data are measured at different scales and thus do not contribute equally to the model fitting. This might create a bias. Thus, the numerical variables have been scaled using min-max normalization. The mathematical formulation of this is:

$$ x_{scaled} = \frac{x-min(x)}{max(x)- min(x)}$$ 


```{r Train data, include= FALSE}
data.train.num <- data.train %>%
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.train.char <- data.train %>%
  select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

min = sapply(data.train.num,min)
range = sapply(data.train.num, range)

data.train.num = (data.train.num-min)/ range

data.train.char <- data.train.char %>%
  map_dfc(as.factor)

data.train.scaled <- cbind(data.train %>% select(age, month, day_of_week, previous), data.train.char, data.train.num)

data.train.scaled <- data.train.scaled %>% 
  select(order(colnames(data.train.scaled)))
```

```{r Test data, include= FALSE}
data.test=data.test %>%
  mutate(month=factor(data.test$month,levels=c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct",
                                                "nov","dec")))
data.test=data.test %>%
  mutate(day_of_week=factor(data.test$day_of_week,levels=c("mon","tue","wed","thu","fri","sat","sun")))

data.test.num <- data.test %>%
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed)

data.test.char <- data.test %>%
  select(campaign, job, marital, education, housing, contact, poutcome, n_contact, y)

data.test.num = (data.test.num-min)/ range

data.test.char <- data.test.char %>%
  map_dfc(as.factor)

data.test.scaled <- cbind(data.test %>% select(age, month, day_of_week,previous), data.test.char, data.test.num)

data.test.scaled <- data.test.scaled %>% 
  select(order(colnames(data.test.scaled)))

```

# Statistical Modelling

## Model 1: Support Vector Machines

Support vector machine is a classification technique used for a binary output variable. In its basic form, SVM is a linear classifier which fits a hyperplane to the data that divides the two classes of the binary output into separate regions. Any new point in either region is classified as such. 

We can use kernels to use non-linear classification with SVM. 

If a linear classification is not possible, or does not give particularly good predictions, we can transform the input variables into a higher dimensional vector space which can be classified using a linear hyperplane in the higher dimensional space and then project the result back onto our original vector space. Projecting the decision boundary in a lower dimension vector space will lead to a non-linear classifier in the original vector space. We can make the calculations easier by transforming the variables with known functions which are called kernels. 

Commonly used kernels are: 

* Linear 
* Polynomial 
* Radial  

All of the above 3 kernels have been considered in this analysis. 

```{r, include= FALSE, eval = FALSE}

#Support Vector Machines
# Values for hyperparameters
set.seed(123)
cost_range <- c(0.01,0.1,1,2,5,10)
degree_range <- 2:4
gamma_range <- c(0.001,0.01,0.1,1,10)

# Since the tune.svm() function is too computationally expensive, we will split our training dataset again into training and validation set to tune hyperparameters for different kernels

# Index for splitting data
n = nrow(data.train.scaled)
sub_idx = sample(c(1:n),n*0.15)
```

So, the training dataset was split into a training set and a validation set for hyperparameter tuning. After running the model for the 3 kernels and different values for the hyper parameter, the model with minimum error was chosen as well as maximum accuracy and sensitivity. 

```{r, eval = FALSE, include = FALSE}
# SVM with linear kernel:

C.error <- numeric(length(cost_range))

for(i in 1:length(cost_range)){
  model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="linear",cost =cost_range[i])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i] <- pred.error(pred.model,data.train.scaled$y)
}

cost_selected = cost_range[which.min(C.error)]

optim.svm.linear = svm(y~.,data = data.train.scaled[-sub_idx], type="C-classification", kernel="linear",cost = cost_selected)


data.train.pred = predict(optim.svm.linear,data.train.scaled[-sub_idx,])
print("Table for linear SVM training data:")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.linear,data.train.scaled[sub_idx,])
print("Table for linear SVM validation data:")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# SVM with polynomial kernel

C.error = matrix(nrow = length(degree_range),ncol = length(cost_range))

for(i in 1:length(degree_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

degree_selected = degree_range[which(C.error == min(C.error), arr.ind=TRUE)[,'row']]

cost_selected = cost_range[which(C.error == min(C.error), arr.ind=TRUE)[,'col']]


optim.svm.poly = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="polynomial",degree = degree_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.poly,data.train.scaled[-sub_idx,])
print("Table for poly SVM training data")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.poly,data.train.scaled[sub_idx,])
print("Table for poly SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# SVM with Radial kernel

C.error = matrix(nrow = length(gamma_range),ncol = length(cost_range))


for(i in 1:length(gamma_range)){
  for(j in 1:length(cost_range)){
    model <- svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_range[i],cost =cost_range[j])
    pred.model <- predict(model,data.train.scaled[sub_idx,])
    C.error[i,j] = pred.error(pred.model,data.train.scaled[sub_idx,]$y)
  }
}

gamma_selected = gamma_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'row'])]

cost_selected = cost_range[min(which(C.error == min(C.error), arr.ind=TRUE)[,'col'])]

optim.svm.radial = svm(y~.,data = data.train.scaled[-sub_idx,], type="C-classification", kernel="radial",gamma = gamma_selected,cost = cost_selected)


data.train.pred = predict(optim.svm.radial,data.train.scaled[-sub_idx,])
print("Table for radial SVM training data")
print(table(data.train.scaled[-sub_idx,]$y,data.train.pred))


data.train.pred = predict(optim.svm.radial,data.train.scaled[sub_idx,])
print("Table for radial SVM validation data")
print(table(data.train.scaled[sub_idx,]$y,data.train.pred))


# The linear and polynomial SVM perform much better than the radial kernel, the radial kernel either overfits the data or classifies all values as 'No' so even with good accuracy , it is not at all a good fit for the data.
```

From the above results, the ‘linear’ kernel with the cost parameter set to ‘0.01’ turned out to gives us the best result for the SVM classifier with an accuracy of 90% and sensitivity of 23%. 

Even though the accuracy of the model is 90%, the sensitivity is low. The reason could be an unbalanced dataset. But a different model might be a better fit for the given data. 

## Model 2: K-Nearest Neighbours

Under k-nearest neighbours classification technique, the k-nearest labelled points predict the class of this point to be the class that most of its neighbours share. 

Here, the value of k has been chosen through Cross-validation approach. Cross validation has been done in two different ways - K-fold cross validation and leave-one-out cross validation. In K- fold cross validation, the data is divided into K roughly equal sized parts. Firstly, the validation data is taken as the first set and training data as all the other sets and the validation error rate/classification rate is estimated for this split. The process is then repeated K-1 more times, with a different part of the data set as the validation data. The final error rate is the average of the K error rates estimated. 

Leave-one-out cross validation is performed entirely on the training data. 

```{r kNN Technique, include= FALSE}
#Preparing the data for kNN
data.kNN.char = data.train.scaled %>%
  select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char = data.kNN.char %>%
  map_dfc(as.numeric)

data.kNN <- cbind(data.train.scaled %>% select(age,previous), data.kNN.char, data.train.num)

data.kNN = data.kNN %>%
  select(order(colnames(data.kNN)))

glimpse(data.kNN)

N=nrow(data.kNN)
div=sample(c(1:N),0.8*N)
data.train.kNN=data.kNN[div,]
data.val.kNN=data.kNN[-div,]

glimpse(data.train.kNN)

classes = data.train.kNN[,18]
classes
classes.val=data.val.kNN[,18]
classes.val

glimpse(data.train.kNN)

#For test data

data.kNN.char.test = data.test.scaled %>%
  select(campaign,contact,day_of_week,education,housing,job,marital,month,n_contact,poutcome,y)

data.kNN.char.test
data.kNN.char.test = data.kNN.char.test %>%
  map_dfc(as.numeric)

data.test.kNN <- cbind(data.test.scaled %>% select(age,previous), data.kNN.char.test, data.test.num)

glimpse(data.test.kNN)

data.test.kNN = data.test.kNN %>%
  select(order(colnames(data.test.kNN)))

#K-fold cross validation, to find the right value for k

K=c(1:15)
val.error=c()
for (k in K) {
  val.pred=knn(data.train.kNN, data.val.kNN, classes, k=k)
  val.error[k]=mean(classes.val != val.pred)
}

plot(K, val.error, type="b", ylab="validation error rate")

k.opt <- which.min(val.error)
k.opt

#Predicting the values using train data

train.pred=knn(data.train.kNN, data.val.kNN, classes, k=k.opt)
train.tab=table(data.val.kNN[,18],train.pred)
train.rate=prop.table(train.tab,margin=1)
train.rate
train.tab

Sensitivity=train.tab[2,2]/(train.tab[2,2]+train.tab[2,1])
Sensitivity
Accuracy= (train.tab[1,1]+train.tab[2,2])/N
Accuracy

#Leave one-out-cross validation

K <- c(1:15)
cv.error <- c()
for (k1 in K){
  train.pred <- knn.cv(data.kNN, data.kNN[,18], k=k1)
  cv.error[k1] <- mean(data.kNN[,18] != train.pred)
}

plot(K, cv.error, type="b", ylab="validation error rate")

k.opt2=which.min(cv.error)
k.opt2

#train.pred2 <- knn(data.kNN, data.train.kNN, data.kNN[,18], k=k.opt2)
#train.tab2=table(data.train.kNN[,18],train.pred2)
#train.rate2=prop.table(tab.pred2,margin=1)
#train.rate2

#Sensitivity2=train.tab2[2,2]/(train.tab2[2,2]+train.tab2[2,1])
#Sensitivity2
#Accuracy2= (train.tab2[1,1]+train.tab2[2,2])/N
#Accuracy2

#Comparing the two cross-validations
c(Sensitivity,Accuracy)
#c(Sensitivity2,Accuracy2)

#Out of the two cross-validations, both the sensitivity and accuracy are better in leave one out cross validation. 

```

From the above results, the optimum k value from both the methods is coming out to be 6, however the Accuracy and Sensitivity rates are higher for leave-one-out cross validation. Thus, we would choose model 2 out of the two models.

## Model 3: Decision Tree

## Model 4: Random Forest

## Model 5: Gradient Boosting

Boosting algorithm is a method used to create an ensemble of simple individual models that together create a better model. First, an initial model is fitted to the data. Then a second model is built that focuses on correctly predicting the cases that were incorrectly predicted by the first model. The combination of these two models is better than either of the two models alone. This process of boosting is repeated, with each successive model attempting to correct for the shortcomings of the combined boosted ensemble of all the previous models.  

 

Gradient Boosting is a classification technique that uses this boosting algorithm. The word gradient is used because here the target outcomes for each case are set, based on the gradient (partial derivative of our loss function) of the error with respect to the prediction. This means that the target outcome for each case depends on how much changing that case’s prediction impacts the prediction error. The gradient can be used to find the direction in which to change the model parameters to reduce the error in the next round of training.  

 

When the target variable is continuous, we use Gradient Boosting Regressor and when it is a classification problem, we use Gradient Boosting Classifier. Since our target column is binary, we have used Gradient Boosting Classifier.  

 

Here, we use log likelihood as our loss function. When we differentiate this function, we get log(odds) which is then used to find a value for which the loss function is minimum. This minimum value is the first prediction of our base model. Next, we calculate the pseudo residuals which are then used as target variables for our second model. Finally, we get new predictions by adding this model to our base model. 

 

For our model, we have used XGBoost, which stands for Extreme Gradient Boosting. It is a specific implementation of the Gradient Boosting method which used more accurate approximations to find the best tree model. It computes second order gradients that provides more information about the direction of the gradients and how to get the minimum of our loss function. 

```{r Gradient bossting technique, include= FALSE, eval = FALSE}

#Fitting the model on the training set
set.seed(123)
model.gbm=train(y~.,data=data.train.scaled,method="xgbTree", trControl=trainControl("cv",number=10))
model.gbm$bestTune

pred.class=model.gbm %>%
  predict(data.train.scaled)
pred.class

#Variable Importance
varImp(model.gbm)

#Compute model prediction accuracy rate
mean(pred.class==data.train.scaled[,18])
tab.gbm=table(pred.class,data.train.scaled[,18])
rate.gbm=prop.table(tab.gbm,margin = 1)
rate.gbm

sens=tab.gbm[2,2]/(tab.gbm[2,1]+tab.gbm[2,2])
sens

#Predicting on test data
pred.class.test=model.gbm %>%
  predict(data.test.scaled)
pred.class.test

mean(pred.class.test==data.test.scaled[,18])
tab.gbm.test=table(pred.class.test,data.test.scaled[,18])
rate.gbm=prop.table(tab.gbm.test,margin = 1)
rate.gbm

sens2=tab.gbm.test[2,2]/(tab.gbm.test[2,1]+tab.gbm.test[2,2])
sens2
```

The sensitivity for this model is 68% and the accuracy is almost 90%. Overall, this appears to be a good fit to the data. 

## Model 6: Linear Discriminant Analysis

# Results

# Conclusion




```{r, include= FALSE, eval = FALSE}
#Tree Based Methods- own way

tree <- rpart(y~ ., data = data.train.scaled, method = "class", minsplit = 4, minbucket = 2, cp = 0.04)
rpart.plot(tree, type = 2, extra = 106)

tree$variable.importance
n= ncol(data.train.scaled)

train.pred <- predict(tree, newdata= data.train.scaled,type="class")
table(data.train.scaled[,n], train.pred)

176/(176+661)
(6513+176)/(6513+92+661+176)

data.train.rt.pruned <- prune(tree, cp=0.04)
rpart.plot(data.train.rt.pruned)


```

```{r, include= FALSE, eval = FALSE}
#Tree Based Methods

tune_grid = expand.grid(
  cp = seq(from = 0, to = 0.01, by = 0.001)
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = FALSE, # no training log
  allowParallel = FALSE, # FALSE for reproducible results 
  classProbs = TRUE
)

rpart1_tune = train(
  y ~ .,
  data = data.train.scaled,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "rpart")

ggplot(rpart1_tune) +
  theme(legend.position = "bottom")

tree <- rpart(y ~ ., data = data.train.scaled, cp = rpart1_tune$bestTune)

rpart.plot(tree)
```

```{r, include= FALSE, eval = FALSE}
# plotting importance from predictive models into two panels
fun_imp_ggplot_split = function(model){
  # model: model used to plot variable importances
  
  if (class(model)[1] == "ranger"){
    imp_df = model$variable.importance %>% 
      data.frame("Overall" = .) %>% 
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  } else {
    imp_df = varImp(model) %>%
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  }
  
  # first panel (half most important variables)
  gg1 = imp_df %>% 
    slice(1:floor(nrow(.)/2)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("Variables") +
    ylab("Importance") +
    theme(legend.position = "none")
    
  imp_range = ggplot_build(gg1)[["layout"]][["panel_params"]][[1]][["x.range"]]
  imp_gradient = scale_fill_gradient(limits = c(-imp_range[2], -imp_range[1]),
                                     low = "#132B43", 
                                     high = "#56B1F7")
  
  # second panel (less important variables)
  gg2 = imp_df %>% 
    slice(floor(nrow(.)/2)+1:nrow(.)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("") +
    ylab("Importance") +
    theme(legend.position = "none") +
    ylim(imp_range) +
    imp_gradient
  
  # arranging together
  gg_both = plot_grid(gg1 + imp_gradient,
                      gg2)
  
  return(gg_both)
}


# plotting two performance measures
fun_gg_cutoff = function(score, obs, measure1, measure2) {
  # score: predicted scores
  # obs: real classes
  # measure1, measure2: which performance metrics to plot
  
  predictions = prediction(score, obs)
  performance1 = performance(predictions, measure1)
  performance2 = performance(predictions, measure2)
  
  df1 = data.frame(x = performance1@x.values[[1]],
                   y = performance1@y.values[[1]],
                   measure = measure1,
                   stringsAsFactors = F) %>% 
    drop_na()
  df2 = data.frame(x = performance2@x.values[[1]],
                   y = performance2@y.values[[1]],
                   measure = measure2,
                   stringsAsFactors = F) %>% 
    drop_na()
  
  # df contains all the data needed to plot both curves
  df = df1 %>% 
    bind_rows(df2)
    
  # extracting best cut for each measure
  y_max_measure1 = max(df1$y, na.rm = T)
  x_max_measure1 = df1[df1$y == y_max_measure1, "x"][1]
  
  y_max_measure2 = max(df2$y, na.rm = T)
  x_max_measure2 = df2[df2$y == y_max_measure2, "x"][1]
  
  txt_measure1 = paste("Best cut for", measure1, ": x =", round(x_max_measure1, 3))
  txt_measure2 = paste("Best cut for", measure2, ": x =", round(x_max_measure2, 3))
  txt_tot = paste(txt_measure1, "\n", txt_measure2, sep = "")
  
  # plotting both measures in the same plot, with some detail around.
  gg = df %>% 
    ggplot() +
    aes(x = x,
        y = y,
        colour = measure) +
    geom_line() +
    geom_vline(xintercept = c(x_max_measure1, x_max_measure2), linetype = "dashed", color = "gray") +
    geom_hline(yintercept = c(y_max_measure1, y_max_measure2), linetype = "dashed", color = "gray") +
    labs(caption = txt_tot) +
    theme(plot.caption = element_text(hjust = 0)) +
    xlim(c(0, 1)) +
    ylab("") +
    xlab("Threshold")
    
  return(gg)
}
  
# creating classes according to score and cut
fun_cut_predict = function(score, cut) {
  # score: predicted scores
  # cut: threshold for classification
  
  classes = score
  classes[classes > cut] = 1
  classes[classes <= cut] = 0
  classes = as.factor(classes)
  
  return(classes)  
}

fun_imp_ggplot_split(tree)
```

```{r, include= FALSE, eval = FALSE}
tree_train_score = predict(tree,
                           newdata = data.train.scaled,
                           type = "prob")[, 2]

tree_test_score = predict(tree,
                          newdata = data.test.scaled,
                          type = "prob")[, 2]

measure_train = fun_gg_cutoff(tree_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.25, 0.5), 
             linetype = "dashed")
```

```{r, include= FALSE, eval = FALSE}

data.train.scaled_1 = data.train.scaled %>% 
  mutate(y = factor(if_else(y == "yes", "1", "0"), 
                    levels = c("0", "1")))
tree_train_cut = 0.25
tree_train_class = fun_cut_predict(tree_train_score, tree_train_cut)
tree_train_confm = confusionMatrix(tree_train_class, data.train.scaled_1$y, 
                                   positive = "1",
                                   mode = "everything")
tree_train_confm

```

```{r, include= FALSE, eval = FALSE}
#Random Forest

tune_grid = expand.grid(
  mtry = c(1:(floor(ncol(data.train.scaled) * 0.7))),
  splitrule = c("gini", "extratrees"),
  min.node.size = 1
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = FALSE, # no training log
  allowParallel = FALSE, # FALSE for reproducible results 
  classProbs = TRUE
)

ranger_tune = train(
  y ~ .,
  data = data.train.scaled,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "ranger"
)

str(data.train.scaled)

ggplot(ranger_tune) +
  theme(legend.position = "bottom")

rf = ranger(y ~ .,
            data = data.train.scaled,
            num.trees = 1000,
            importance = "impurity",
            splitrule = ranger_tune$bestTune$splitrule,
            mtry = ranger_tune$bestTune$mtry,
            min.node.size = ranger_tune$bestTune$min.node.size,
            write.forest = T,
            probability = T)

```

```{r, include= FALSE, eval = FALSE}
print(rf)
```

```{r, include= FALSE, eval = FALSE}
fun_imp_ggplot_split(rf)
```

```{r, include= FALSE, eval = FALSE}
rf_train_score = predict(rf,
                         data = data.train.scaled)$predictions[, 2]

rf_test_score = predict(rf,
                        data = data.train.scaled)$predictions[, 2]

measure_train = fun_gg_cutoff(rf_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
measure_train = fun_gg_cutoff(rf_train_score, data.train.scaled$y, 
                              "acc", "f")
measure_train +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
```

```{r, include= FALSE, eval = FALSE}
rf_train_cut = 0.3
rf_train_class = fun_cut_predict(rf_train_score, rf_train_cut)
# matrix
rf_train_confm = confusionMatrix(rf_train_class, data.train.scaled_1$y, 
                                 positive = "1",
                                 mode = "everything")
rf_train_confm

```

```{r, eval=TRUE}
#LDAQDA

#data preparation to apply PCA to continuous correlated variables
data.train.num <- data.train.scaled %>% dplyr::select(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed)

data.train.num.pca <- princomp(data.train.num, cor = T)

print(summary(data.train.num.pca))
#the first component accounts for 99.8% variation

#atleast 80% variance to be retained.
#using cumulative proportion values to choose the components

data.train.scaledLda <- data.train.scaled %>% bind_cols(data.train.num.pca$scores[, 1:3]) %>% select(-c(cons.conf.idx, cons.price.idx, emp.var.rate, euribor3m, nr.employed))


```

```{r LDA, eval=TRUE}
library(MASS)

#start with a simpler model
#include the variables that deemed most imp. during exploratory analysis
lda.fit1 <- lda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)


```

```{r, eval=TRUE}

#dataframe containing actual, predicted values and decision scores
#all evaluated on training set.
#Cross validation to be done at a later stage
predict.data <- data.train.scaledLda %>% dplyr::select(y)
predict.data['y_lda1'] <- predict(lda.fit1)$class
predict.data['y_lda1_probs'] <- predict(lda.fit1)$posterior[, 'yes']

confusionMatrix(predict.data$y_lda1, predict.data$y, positive = 'yes', mode = 'everything')

```



```{r autoeval, eval=TRUE}
#auto_eval() will add variables to the model one by one and compare the model's
#sensitivity and precision. If any added variable gives improvement of over 1% point,
#it will display the formula for that model. It does not stop iteration if error
#occurs during model fitting

auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  lda1_sensi <- 0.21
  lda1_preci <- 0.656
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(lda.fit.trial <- lda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(lda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > lda1_sensi + 0.1 | preci > lda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```


After the first model, a trial and error approach was taken to select the variables to add to the model. None of the variables provided better prediction with respect to all the metrics. Hence the optimal Linear discriminant model was found to be lda.fit1

```{r QDA, eval=TRUE}
#the initial QDA model uses the same variables as LDA
qda.fit1 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome, data = data.train.scaledLda)
```

```{r predictdataqda, eval=TRUE}

#data frame containing predicted, actual values and decision scores
#for qda
predict.data['y_qda1'] <- predict(qda.fit1)$class
predict.data['y_qda1_probs'] <- predict(qda.fit1)$posterior[, 'yes']

confusionMatrix(predict.data$y_qda1, predict.data$y, positive = 'yes', mode = 'everything')
```
Same trial and error process was adopted.

```{r, eval=TRUE}

auto_eval <- function(){
  included <- c('age', 'Comp.1' , 'Comp.2' , 'Comp.3' , 'n_contact' , 'poutcome')
  col.names <- colnames(data.train.scaledLda)[!colnames(data.train.scaledLda) %in% append(included, 'y')]
  
  qda1_sensi <- 0.217
  qda1_preci <- 0.65
  
  formula_store <- vector(mode = 'list')
  
  for(i in 1:length(col.names)){
    print(i)
    covariates <- append(included, col.names[i])
    formula <- formula(paste('y', paste(covariates, collapse = ' + '), sep = ' ~ '))
    
    res <- try(qda.fit.trial <- qda(formula, data = data.train.scaledLda))
    
    if(inherits(res, 'try-error')){
      print(paste(col.names[i], 'error', sep = ' '))
      next
    }
    
    #uses caret
    sensi <- recall(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    preci <- precision(predict(qda.fit.trial)$class, predict.data$y, relevant = 'yes')
    
    if(sensi > qda1_sensi + 0.1 | preci > qda1_preci + 0.1){
      formula_store <- append(formula_store, formula)
    }
  }
  formula_store
}

auto_eval()
```

```{r qda.fit2, eval=TRUE}
#job variable detected by auto_eval
#job variable added
qda.fit2 <- qda(y ~ age + Comp.1 + Comp.2 + Comp.3 + n_contact + poutcome + job, data = data.train.scaledLda)

```

```{r, eval=TRUE}
predict.data['y_qda2'] <- predict(qda.fit2)$class
predict.data['y_qda2_probs'] <- predict(qda.fit2)$posterior[, 'yes']

confusionMatrix(predict.data$y_qda2, predict.data$y, positive = 'yes', mode = 'everything')

#Sensitivity for qda.fit2 increased but precision declined; Precision recall trade-off
```

.

```{r CV , eval=TRUE}

#cross validation results for all three models i.e.
#lda.fit1, qda.fit1, qda.fit2

set.seed(169)
folds <- createFolds(data.train.scaledLda$y, k = 10)

lda1_cv_sensi <- vector(mode = 'numeric', length = 10)
lda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda1_cv_sensi <- vector(mode = 'numeric', 10)
qda1_cv_preci <- vector(mode = 'numeric', length = 10)

qda2_cv_sensi <- vector('numeric', 10)
qda2_cv_preci <- vector('numeric', 10)

for(i in 1:length(folds)){
  train <- data.train.scaledLda[-folds[[i]], ]
  test <- data.train.scaledLda[folds[[i]], ]
  
  lda1 <- lda(formula(lda.fit1$call), data = train)
  qda1 <- qda(formula(qda.fit1$call), data = train)
  qda2 <- qda(formula(qda.fit2$call), data = train)
  
  lda1_cv_sensi[i] <- recall(predict(lda1, test)$class, test$y, relevant = 'yes')
  lda1_cv_preci[i] <- precision(predict(lda1, test)$class, test$y, relevant = 'yes')
  
  qda1_cv_sensi[i] <- recall(predict(qda1, test)$class, test$y, relevant = 'yes')
  qda1_cv_preci[i] <- precision(predict(qda1, test)$class, test$y, relevant = 'yes')
  
  qda2_cv_sensi[i] <- recall(predict(qda2, test)$class, test$y, relevant = 'yes')
  qda2_cv_preci[i] <- precision(predict(qda2, test)$class, test$y, relevant = 'yes')
              
}

metric.frame <- data.frame(avg_sensi = c(mean(lda1_cv_sensi), mean(qda1_cv_sensi), mean(qda2_cv_sensi)),
                           avg_preci = c(mean(lda1_cv_preci), mean(qda1_cv_preci), mean(qda2_cv_preci)),
                           row.names = c('lda1', 'qda1', 'qda2'))

metric.frame
```


